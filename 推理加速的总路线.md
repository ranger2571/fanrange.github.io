
# 2024年初的文章：
作者：刀刀宁  
链接：https://www.zhihu.com/question/591646269/answer/3380020160  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  
  

切入大模型推理技术的时间，到现在满打满算一个季度了，正好利用过年前整理一下最近学习的进度。

目标是快速的对整个推理技术做一个整体全面的调研和学习，把相关技术的整体基础打好，然后找到一些有效的研发突破点。

因为我之前是做视觉，也有很大一块在研究如何在轻量化设备上部署和加速CNN卷积模型的推理过程，除了芯片加速之外量化稀疏化裁剪蒸馏训练。当下的大模型虽然可以融合多模态但是目前其实还是以NLP应用为主，不过底层的加速逻辑是完全相通的，所以在学习过程中会有所比较和引申，加深学习效果。

目前也是比较初步的学习，体系搭建了一部分但是内容还不是很多。

---

最开始的切入点是DeepSpeed inference和llama.cpp两个项目，跑起来都很容易，代码阅读难度也不大。写了几篇阅读笔记：[刀刀宁：笔记：DeepSpeed inference 代码理解](https://zhuanlan.zhihu.com/p/668181423) [刀刀宁：笔记：Llama.cpp 代码浅析（一）：并行机制与KVCache](https://zhuanlan.zhihu.com/p/670515231) [刀刀宁：笔记：Llama.cpp 代码浅析（二）：数据结构与采样方法](https://zhuanlan.zhihu.com/p/671761052) [刀刀宁：笔记：Llama.cpp 代码浅析（三）：计算开销](https://zhuanlan.zhihu.com/p/672289691) 。当然对于这两个很庞大源码的项目， 读到的部分还是比较肤浅的。这里最大的帮助是，梳理了推理时大体的流程，以及网络结构、计算图的构建、算子调度、算子细节、当下的基本瓶颈、可能的优化细节等等问题。

同时还可以抽取出来一些较为关键的技术问题：张量并行机制、KVCache 原理、Beam Search等投机推理方法。但是这对于整个大模型推理所涉及到的整个技术面还有好几块大的拼图没有凑齐。

接下来就是沿着我涉及到的有这么几大块内容，逐个展开，其中一部分内容会进行深入的研究：

**一）**单request单次推理中的算子级优化问题，如算子融合、算子加速，前者如我在 [算子融合是什么意思？](https://www.zhihu.com/answer/3379660344) 中举的栗子，后者主要就是以[FlashAttention](https://zhida.zhihu.com/search?content_id=644625376&content_type=Answer&match_order=1&q=FlashAttention&zhida_source=entity)、[FlashDecoder](https://zhida.zhihu.com/search?content_id=644625376&content_type=Answer&match_order=1&q=FlashDecoder&zhida_source=entity)为代表工作。核心问题还是在矩阵分块的时候设计好一个能够充分利用高速访存HBM的分块方法，让一次搬运进HBM中的参数可以全部和该做乘加操作的算子都计算完再丢弃，达到数量单次访存利用率最大化。

**二）**面向多request多用户推理服务系统中的优化问题，这里通俗的讲就是说如果当个算子优化的再好，但是用户在使用时都是排队等待前面用户任务执行完再跑自己的，那估计系统服务就没有人用了。并且大模型推理中潜在的访存墙问题也会因为多服务合并出来的大batch，每次共享（复用）读取到的参数，增加单位访存的利用率，进而增加芯片计算利用率。目前来说比较有代表性的工作就是[continuing batch](https://zhida.zhihu.com/search?content_id=644625376&content_type=Answer&match_order=1&q=continuing+batch&zhida_source=entity)等工作。

我在学习过程中写过一个回答：[LLM推理相比普通transformer推理有什么特殊性？](https://www.zhihu.com/answer/3355733107) 不过我当时回答的有些问题，混淆了多请求服务和分布式系统之间的逻辑，最近我在重新梳理多推理服务化系统的一些技术特点，到时候弄完也会发一篇博客出来吧。

**三）**当模型大到一块卡甚至一台机器放不下的情况，包括模型放得下但是多请求服务系统中形成的规模放不下的情况，都会涉及到分布式系统和分布并行方法来帮忙解决其中的瓶颈。包括并行机制、分布调度、通信优化、容错优化等等方面。

**四）**改进推理算法类的工作：比如在推理时不再是一个词一个词蹦，可能一次多出来几个词作为候选的投机推理方法，比较有名如SpecInfer等工作；还有改变自回归机制的方法。不过这些方法大体上都需要训练配合。还有就是新的循环结构，如[RWKV](https://zhida.zhihu.com/search?content_id=644625376&content_type=Answer&match_order=1&q=RWKV&zhida_source=entity)等。

**五）**模型小型化的工作：这里又会细分为小模型设计、模型稀疏化、模型剪枝、量化推理、蒸馏等方面。这里也形成了几篇论文阅读笔记：[动态剪枝方法：论文笔记：DejaVu、LLM in Flash、PowerInfer](https://zhuanlan.zhihu.com/p/675585887) [静态稀疏化方法，矩阵分解：Low-Rank Pruning of Llama2](https://zhuanlan.zhihu.com/p/678891209) [静态稀疏化方法，模型裁剪：再看大模型稀疏化：SparseGPT、Wanda](https://zhuanlan.zhihu.com/p/679376718) 。接下来还会把量化再梳理一下。其实这块整体的思路和我们之前在视觉领域所做的小型化轻量化的工作，是没有太大本质上的区别的，现在还需要在大语言模型的基础上找到差异性和新特点，找到突破口。

---

在Arxiv上23年12月份的推理服务系统优化综述文章 [arxiv:2312.15234：Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2312.15234) ，这篇文章有315篇参考文献，图中的分类也将是个经典，也不外乎就是我这个学习路径了，大的拼图也就是这么几大块。![[v2-5db77cb06ff3f353f3e053f404bd1a0a_720w.webp]]


论文中还提出了未来大模型推理技术可能的演化方法，这挺值得关注的：

_这就是当下拼图中后边的几大块了。_

**专用硬件加速器**：从我的感受看现在GPU上因为高速缓存的访存速度非常快，所以目前的优化瓶颈都在如何把计算算力利用好，同时相比Attention部分，MLP已经在生态中被优化了好多年了，Attention部分的优化，包括KVCache结合部分，才是真正的瓶颈。同时新的硬件研发也会结合这块的技术特点。

但是，其实，新的硬件研发除了英伟达可以做的很好之外，其他家还有很重要的工作就是需要先赶上GPU在推理所有环节中的其他方面。也就是说，除了专用芯片上的加速之外，还有很大一块强需求点是在相对较弱的芯片或者加速器上把在GPU上看起来不起眼的优化点优化好的问题。

**高效有效的解码算法和探索替代基础架构**​​​​：现在主要的问题还是除了decoder这个结构之外其他的结构算法效果都不太好，但是其实不排除啥时候出现新的结构，怕就是突然的变化让现在基于decoder结构优化的方法存在一定的失效。

算法方面的加速工作我个人是比较看好投机推理的，综述文章中也是这样认为的。因为多保留了几条输出可能，在相同的推理步数下效果显然会比单个序列要好，同时带来的计算量虽然增加了但是因为可以公用相同的参数形成更大的推理batch，有效利用芯片计算资源，甚至都不会增加计算时间。

至于Attention结构会不会出现革命性的改变，这个就看头部大佬们的速度了。_内心OS：不要太快，跟不上啊。_

**长上下文/序列场景优化**​​：这个确实是个值得关注的算法和系统优化点，目前算法优化还不是足够强大，特别长的输入和输出算法都还没有完美的搞定，甚至还轮不到加速这块上场。不过随着算法的进步，新的加速方法也是要跟得上的。

**在复杂环境中的部署探索**和**特定需求的自动适应**​​：随着LLM应用的发展，还需要考虑如何优化它们在各种复杂环境中的部署问题，如云环境、边缘计算环境、云+端混合计算环境、甚至是去中心化计算等。另外就是，针对不同应用领域的多样化需求也有一些研究方向可以思考，比如在CV领域的AutoML、模型结构搜索、模型微调、向量数据库检索、多模态负载等等。

再比如还有一类推理工作可能大家看不到，就是不是给普通用户实时使用的推理系统。比如在RLHF训练过程中需要的批量推理过程、以及预测性的批量推理、用于蒸馏训练的推理过程等等任务。这些任务可以被视为是一种离线推理任务，强调高吞吐量，对于延时不敏感。


# 25年初的文章：
[笔记：推理加速专栏之甲辰年终碎念 - 知乎](https://zhuanlan.zhihu.com/p/9483582772)
时光如逝岁月如梭，又到了年底，又要写年终总结了。这里有七月份的 [学习推理加速半年之总结与迷思](https://zhuanlan.zhihu.com/p/704938096)，去年十二月的 [刀刀宁：整理一下最近学习进度](https://zhuanlan.zhihu.com/p/680314529) ，外加九月份的 [关于 LLM MLSys 研究的一些思考](https://zhuanlan.zhihu.com/p/720634180)。在 [刀刀宁聊大模型推理](https://www.zhihu.com/column/c_1796502192443777024) 这个专栏中，总数上形成了 60 多篇博客和笔记，还有一些零星的问题回答，知乎数据是 49 篇文章 49 个回答。对大模型推理领域的大部分技术有了一定的了解和思考，因此在很多论文阅读的笔记中夹杂了不少私货。**肯定得夹带私货啊，不然和 GPT 又有什么区别？**

前两天也看了不少对于 2025 年大模型发展的预测，预测这个事情吧，我们每年都会做每年都会乐此不疲。我本来也想预测一波来着，但是最后还是忍住了，还是把总结写好，仰望星空不如回看历史。
### 一）大方向是不会变的，给推理加速

在前面规划的量化、服务、投机推理，几个大点之后，重点学习了[长上下文 attention 机制](https://zhida.zhihu.com/search?content_id=250919563&content_type=Article&match_order=1&q=%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87+attention+%E6%9C%BA%E5%88%B6&zhida_source=entity)的加速技术方法，以及 RAG 相关的技术。CoT 实在是跟不上了，战略性的放（）下（qi）了。

总的来说，聚焦的层面既不是很偏向应用层，也不是特别偏向于底层优化。期望是找到结合点，往好里说叫软硬协同（[算法硬件协同](https://zhida.zhihu.com/search?content_id=250919563&content_type=Article&match_order=1&q=%E7%AE%97%E6%B3%95%E7%A1%AC%E4%BB%B6%E5%8D%8F%E5%90%8C&zhida_source=entity)），集成优化系统融合，协同进化优势互补。往不好听里说叫，上不接天下不着地。

这个 [2024年你最喜欢的MLSys相关的工作是什么](https://www.zhihu.com/answer/61827348015) 的回答刚刚好也是一种技术层面的总结。反正是，能做的方向哪一个都是卷到死，哪一个都是人满为患，只能迎难而上了。

  

### 二）好奇心害死人

大模型的大字，不止体现在参数量、计算量、数据量，还在于涉及的技术链路大（长），是真的涉及面又广又宽，每个技术的深度还很深，更新速度还非常的快。如果你对每个技术点都感兴趣，那真的是弄不完的，就算是弄完了这块，又有新的出来了。

正所谓，

吾生也有涯，而知也无涯。以有涯随无涯，殆已。

既要保持好奇心，又要克制好奇心。论文越看越多，其实也越来越乱，该收敛要收敛。不过随着模型 scaling law 这条边际递减曲线趋平之后，可能整个领域也会在未来几年形成一定的收敛效应，说人话就是新东西出来的速率会慢下来，吧。

  

### 三）寻找算法系统结合点

坚持系统类的研究工作的同时，还要兼顾算法类的研究。其实与其说是兼顾，不如说是，只有些接合部的地方才能找到适合我们研究的点。做研究最后还是要打到细节环节上，也没必要太宏观。当然，要是那么容易找到的话，也就不需要做什么研究工作了。找到一个结合点，其实说是大海捞针也完全不为过。

痛并快乐着，吧。

其实，因为分工的原因，做系统的人可能会天然排斥算法细节，不是主观上排斥而是客观上可能没办法，或者说“你算法做成什么样子我就给你优化什么东西”。而做算法的人，是不太关心底层的。而对于每个分工的人，这样肯定是最容易推进的，当然这里面可能不光是技术问题，较为传统的情况下互相关心甚至会被视为是互抢饭碗的嫌疑。

但是其实这不是最优的，为了很多公司和团队会提软硬协同，算法系统协同，算法需要跟着系统的特点进行设计，系统也会跟着算法的演进进行迭代。这其实是个趋势。虽然难，但是我想我还是想坚持这样的思路。

  

### 四）关于系统

算力是一个“黑洞”，再多的硬件投入也满足不了人们日益增长的需求，但是还是要不断投入。而站在研究的角度来说，选择研究方向还是一定要和平台能够组织起的资源相匹配，system 领域的研究则更加明显。

我个人想坚持的是，在力所能及的范围内，要有小范围的落地系统，规模可以不大，但是技术前沿度还是有必要跟得上。也是我这段时间调研除底层加速之外偏应用层几个技术的核心原因，目前落地一套[全栈对话系统](https://zhida.zhihu.com/search?content_id=250919563&content_type=Article&match_order=1&q=%E5%85%A8%E6%A0%88%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F&zhida_source=entity)的难度可以说已经降到了史上最低，但是还是需要花精力在调研和选型中间的每个部件，思考每个逻辑环节的设计初衷和核心思路。也希望在实际落地系统中找到比较巧的研究结合点。

且行且珍惜。

而完整的落地系统，讲真，落地和研究，九成情况下是冲突的，就是花精力做了落地就不好搞研究，反之亦然。倒不是说两者就真的那么互斥，而是以国内绝大多数研究机构的角色功能定位来说，面对大模型这样的重资金重人才重人力重开发且重试错的研究方法来说，只能说核心 KPI 导向不同，不会有研究机构的领导愿意去赌，话说跟随策略它不香吗？剩下的一成，要么是财大气粗的 Lab，要么大概率也得是大厂和明星创业公司在一边实践试错，一边做一些研究机构应该做的事情，并且开源代码和 paper。

  

### 五）坚持写写写

文章写了很多，当然是以论文阅读笔记为主，论文阅读又是夹杂着大量私货为主。当然，夹带私货还是需要些许自信，因为也是怕自己的逻辑和判断有误。

大胆判断，小心求证。

其实，有的时候也是要相信自己的第一直觉，我们经常会把研究中的第一直觉称为研究嗅觉。别小看这个直觉，好像不靠谱似得，其实不然，因为从[丹尼尔卡尼曼](https://zhida.zhihu.com/search?content_id=250919563&content_type=Article&match_order=1&q=%E4%B8%B9%E5%B0%BC%E5%B0%94%E5%8D%A1%E5%B0%BC%E6%9B%BC&zhida_source=entity)思考快与慢的角度说，直觉就是直接动用了快思维那部分功能，而这部分功能是经年累月的信息汲取和专业训练之后的慢思维部分“转化”过来的。普通人的快思维不靠谱，但是专业性的快思维，搞不好是有奇效的。就好像于谦的醉酒版汾河湾，快思维输出的东西，一定不能深究其对错，但是却是某种神来之笔。[知乎想法](https://www.zhihu.com/pin/1858596107245973505)

所以，写作的私货，很大可能就是我的汾河湾，某个灵机一现的点子。

  

这里还是要说，写下来的过程，除了点子，还是通过这种方式督促自己把所看所思所想的内容梳理清晰，按自己的理解再重新输出一次，并找到问题。反正也是要在自己的笔记中写写写的，能公开的部分干脆就在知乎上写，一方面帮自己整理，另一方面也可以形成和知乎小伙伴的互动。挺好。时不时的需要把笔记拿出来再思考，再砸吧砸吧。

  

### 六）结语

今年是甲辰年，四十年前的 1984 年甲子年，是天干地支循环的一个开端，也是中国民营企业的元年。那一年，很多知名企业纷纷诞生，开启了中国民营经济蓬勃发展的新时代，如今这些企业已走过四十载风雨，经历了各种机遇与挑战。而当下通用人工智能技术蓬勃发展、方兴未艾，新技术和新产品层出不穷，新论文更是眼花缭乱看不过来，各种工具更加改变了我们的工作方式，不管是帮忙读论文、帮忙写代码，还是帮忙写笔记，其实都已经离不开智能对话小助手了，都不光是效率问题了，是离不开了。从更大的层面上看，这些技术在未来几年将深刻的改变全社会各个行业，正如四十年前的民营企业在改革开放的浪潮中崛起一样，如今的业界和学界，也都将在人工智能的浪潮中寻找新的发展机遇，所以，虽然路长且阻，但是还是要打起精神，摩拳擦掌，来年再战。

最后还是祝各位读到这里的读者小伙伴们，新年快乐！来年再接再厉，共创辉煌。

---

最后附上这一年的专栏全集（一般笔记不敢附全集，按类型分类，有重复所以显得多）：

盘点类：

- [刀刀宁：眼花缭乱的 2023 与人工智能](https://zhuanlan.zhihu.com/p/675527271)
- [刀刀宁：整理一下最近学习进度](https://zhuanlan.zhihu.com/p/680314529)
- [刀刀宁：笔记：学习推理加速半年之总结与迷思](https://zhuanlan.zhihu.com/p/704938096)
- [刀刀宁：笔记：关于 LLM MLSys 研究的一些思考](https://zhuanlan.zhihu.com/p/720634180)
- [刀刀宁：笔记：推理加速专栏之甲辰年终碎念](https://zhuanlan.zhihu.com/p/9483582772)
- 刀刀宁：[2024年你最喜欢的MLSys相关的工作是什么?](https://www.zhihu.com/answer/61827348015)

视觉类：

- [刀刀宁：BEV 纯视觉感知算法笔记](https://zhuanlan.zhihu.com/p/633624413)
- [刀刀宁：笔记：一个自动驾驶多模态新范式](https://zhuanlan.zhihu.com/p/663315667)
- [刀刀宁：笔记：GAIA-1、DriveDreamer 与世界模型](https://zhuanlan.zhihu.com/p/663643478)
- [刀刀宁：大模型时代的算法研发（在 segment anything 发布之后）](https://zhuanlan.zhihu.com/p/619797754)
- [刀刀宁：Diffusion模型推理过程中的Cache](https://zhuanlan.zhihu.com/p/689685429)
- [刀刀宁：笔记：简单回顾一下机器视觉领域推理加速技术](https://zhuanlan.zhihu.com/p/713916261)
- [刀刀宁：视觉 DETR 模型的蒸馏技术笔记](https://zhuanlan.zhihu.com/p/716123173)

开源代码类：

- [刀刀宁：笔记：DeepSpeed inference 代码理解](https://zhuanlan.zhihu.com/p/668181423)
- [刀刀宁：笔记：Llama.cpp 代码浅析（一）：并行机制与KVCache](https://zhuanlan.zhihu.com/p/670515231)
- [刀刀宁：笔记：Llama.cpp 代码浅析（二）：数据结构与采样方法](https://zhuanlan.zhihu.com/p/671761052)
- [刀刀宁：笔记：Llama.cpp 代码浅析（三）：计算开销](https://zhuanlan.zhihu.com/p/672289691)
- [刀刀宁：笔记：Llama.cpp 代码浅析（四）：量化那些事](https://zhuanlan.zhihu.com/p/672983861)

参数稀疏化：

- [刀刀宁：论文笔记：DejaVu、LLM in Flash、PowerInfer](https://zhuanlan.zhihu.com/p/675585887)
- [刀刀宁：Low-Rank Pruning of Llama2](https://zhuanlan.zhihu.com/p/678891209)
- [刀刀宁：再看大模型稀疏化：SparseGPT、Wanda](https://zhuanlan.zhihu.com/p/679376718)
- [刀刀宁：再磕：GPTQ、SparseGPT与Hessian矩阵](https://zhuanlan.zhihu.com/p/680578625)
- [刀刀宁：聊聊大模型推理的动态稀疏化之二：MInference1 与 PowerInfer2](https://zhuanlan.zhihu.com/p/711399792)

参数与缓存量化类：

- [刀刀宁：量化那些事之FP8与LLM-FP4](https://zhuanlan.zhihu.com/p/683215538)
- [刀刀宁：量化那些事之 AWQ 与 SmoothQuant](https://zhuanlan.zhihu.com/p/684215316)
- [刀刀宁：量化那些事之QARepVGG](https://zhuanlan.zhihu.com/p/684297101)
- [刀刀宁：量化那些事之BitNet-b1.58](https://zhuanlan.zhihu.com/p/684658121)
- [刀刀宁：笔记：Llama.cpp 代码浅析（四）：量化那些事](https://zhuanlan.zhihu.com/p/672983861)
- [刀刀宁：量化那些事之KVCache的量化](https://zhuanlan.zhihu.com/p/691537237)
- [刀刀宁：量化那些事之 ViT 的量化](https://zhuanlan.zhihu.com/p/693636710)
- [刀刀宁：量化那些事之 Diffusion 量化](https://zhuanlan.zhihu.com/p/693474068)
- [刀刀宁：量化那些事之 AdaRound/BRECQ/QDrop](https://zhuanlan.zhihu.com/p/696542303)
- [刀刀宁：量化那些事之 QServe](https://zhuanlan.zhihu.com/p/697465029)
- [刀刀宁：量化那些事之蒸馏 QAT](https://zhuanlan.zhihu.com/p/702976139)
- [刀刀宁：量化那些事之 OmniQuant/SpinQuant 等](https://zhuanlan.zhihu.com/p/703301768)
- [刀刀宁：量化那些事之llm.int8/SpQR/RPTQ](https://zhuanlan.zhihu.com/p/703861580)
- [刀刀宁：量化那些事之性能评价指标](https://zhuanlan.zhihu.com/p/697297550)

推理服务类：

- [刀刀宁：聊聊大模型推理服务中的优化问题](https://zhuanlan.zhihu.com/p/677650022)
- [刀刀宁：聊聊大模型推理服务之长上下文](https://zhuanlan.zhihu.com/p/698138500)
- [刀刀宁：聊聊大模型推理服务之投机推理](https://zhuanlan.zhihu.com/p/699166575)
- [刀刀宁：聊聊大模型推理中的分离式推理](https://zhuanlan.zhihu.com/p/706469785)
- [刀刀宁：聊聊大模型 MaaS 服务](https://zhuanlan.zhihu.com/p/715954241)
- [刀刀宁：笔记：几篇大模型服务系统优化文章](https://zhuanlan.zhihu.com/p/721849501)

投机推理类：

- [刀刀宁：聊聊大模型推理服务之投机推理](https://zhuanlan.zhihu.com/p/699166575)
- [刀刀宁：投机推理番外一：特征层 speculative decoding](https://zhuanlan.zhihu.com/p/704755926)
- [刀刀宁：投机推理番外二：优化树结构](https://zhuanlan.zhihu.com/p/707596864)
- [刀刀宁：投机推理番外三：级联投机](https://zhuanlan.zhihu.com/p/709943483)
- [刀刀宁：投机推理番外四：batch 与 IO](https://zhuanlan.zhihu.com/p/716488006)
- [刀刀宁：投机推理番外五：异构推理](https://zhuanlan.zhihu.com/p/13634441442)

长上下文优化类（含 KVCache 压缩）：

- [刀刀宁：聊聊大模型推理内存管理之 CachedAttention/MLA](https://zhuanlan.zhihu.com/p/707190620)
- [刀刀宁：聊聊大模型推理中的 KVCache 压缩](https://zhuanlan.zhihu.com/p/708946312)
- [刀刀宁：聊聊大模型推理中 KVCache 压缩方法的性能评测](https://zhuanlan.zhihu.com/p/709684237)
- [刀刀宁：聊聊大模型推理中的 Prompt Compression](https://zhuanlan.zhihu.com/p/710508401)
- [刀刀宁：聊聊大模型推理的动态稀疏化之二：MInference1 与 PowerInfer2](https://zhuanlan.zhihu.com/p/711399792)
- [刀刀宁：聊聊大模型推理中的 KVCache 之异构缓存](https://zhuanlan.zhihu.com/p/714288577)
- [刀刀宁：聊聊大模型推理中的 KVCache 异构缓存之二](https://zhuanlan.zhihu.com/p/715921106)
- [刀刀宁：笔记：长上下文优化方法之“用短上下文模型处理长上下文问题”](https://zhuanlan.zhihu.com/p/11028074459)

RAG 类：

- [刀刀宁：笔记：浅读几篇 RAG 推理加速的论文](https://zhuanlan.zhihu.com/p/5505114826)
- [刀刀宁：笔记：浅聊一下 RAG 与长上下文之争](https://zhuanlan.zhihu.com/p/8280024582)
- [刀刀宁：笔记：RAG 的部分优化方法之一](https://zhuanlan.zhihu.com/p/9250009518)
- [刀刀宁：笔记：RAG 的部分优化方法之二（加速篇）](https://zhuanlan.zhihu.com/p/9368910073)
- [刀刀宁：笔记：RAG 的优化方法之三（整理一下）](https://zhuanlan.zhihu.com/p/11728160133)
- [刀刀宁：笔记：RAG 的相关优化方法之四（BGE 家族）](https://zhuanlan.zhihu.com/p/14273316699)
- [刀刀宁：笔记：长上下文优化方法之“用短上下文模型处理长上下文问题”](https://zhuanlan.zhihu.com/p/11028074459)

部分热门技术：

- [刀刀宁：笔记：简单图解一下线性注意力机制](https://zhuanlan.zhihu.com/p/718156896)
- [刀刀宁：笔记：关于 CoT 和推理加速](https://zhuanlan.zhihu.com/p/721557361)
- [刀刀宁：闲聊：关于大模型 Workflow 一点随感](https://zhuanlan.zhihu.com/p/17321332316)
- [刀刀宁：笔记：盘点一下大语言模型中的小模型](https://zhuanlan.zhihu.com/p/16804336778)