[模型压缩-神经网络量化基础 - 知乎](https://zhuanlan.zhihu.com/p/505570612)
[QLoRA、GPTQ：模型量化概述 - 知乎](https://zhuanlan.zhihu.com/p/646210009)
# 量化概念
大模型推理服务极其消耗显存和算力。此外，在多机多卡的场景下，推理过程中 GPU 间的通讯量大，IO 成为了瓶颈，这使得大模型推理的 latency 变得难以接受。

面对上述挑战，模型量化技术能够提供一种解决方案，它能够在推理过程降低资源消耗，提升运行速度，降低延迟。这使得提供大规模的大模型推理服务成为现实。

### **模型量化是做什么的？**

**模型量化是将浮点数值转化为定点数值，同时尽可能减少计算精度损失的方法。**

具体而言，模型量化是一种压缩网络参数的方式，它将神经网络的参数（weight）、特征图（activation）等原本用浮点表示的量值换用定点（整型）表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。

**模型量化实现建立在深度网络对噪声具有一定的容忍性上**，模型量化相当于对深度网络增加了一定的噪声（量化误差），如果量化位数合适，模型量化基本不会造成较大的精度损失。

### **为什么要做模型量化？**

`**模型量化既能减少资源消耗，也能提高运行速度，使大规模推理服务的性能提升。**`

模型量化的好处主要有：

1. 可以减少内存和显存占用，给模型瘦身，降低大模型的使用门槛和资源消耗；

2. 能够提高运行速度，这可以从两方面理解：
- 在适配低精度的硬件下，量化模型的运算能直接用 int8 GEMM kernel 计算；
- 量化减少了单位数据的 bit 数，因而可以减少计算过程中的 IO 通信量。

由于以上两点，我们做模型推理时，可以增加更多的 batch size，同时也能加快计算速度，因此规模化的模型推理就能既快速又高效。

### **对哪些数值做量化？**

**可以对模型参数（weight）、激活值（activation）或者梯度（gradient）做量化。**

通常而言，模型的参数分布较为稳定，因此对参数 weight 做量化较为容易。

然而，模型的激活值往往存在异常值，直接对其做量化，会降低有效的量化格点数，导致精度损失严重，因此，激活值的量化需要更复杂的处理方法（如 SmoothQuant）。

### 常见的量化精度有哪些？

**通常可以将模型量化为 int4、int8 等整型数据格式。**

在大模型方向上，模型的计算一般采用 16-bit 精度（FP16、BF16 等），所以通常我们需要将 int4/int8 转化为 FP16/BF16，然后再进行计算。

如果我们自己实现了 int4/int8 的 cuda kernel，或者 GPU 有 int4/int8 的矩阵运算支持，也可以在低精度下直接运算。

### **量化方法有哪些分类？**

**根据量化方案的不同，可以分为量化感知训练（QAT）和后训练量化（PTQ）。**

- QAT（Quant-Aware Training） 也可以称为**在线量化（On Quantization）**。它需要利用额外的训练数据，在量化的同时结合反向传播对模型权重进行调整，意在确保量化模型的精度不掉点。
- PTQ （Post Training Quantization）也可以称为**离线量化（Off Quantization）**。它是在已训练的模型上，使用少量或不使用额外数据，对模型量化过程进行校准，可能伴有模型权重的缩放。其中：
	- **训练后动态量化（Post****Dynamic Quantization）**不使用校准数据集，直接对每一层 layer 通过量化公式进行转换。**`QLoRA 就是采用这种方法。`**
	- **训练后校正量化（Post Calibration Quantization）**需要输入**有代表性**的数据集，根据模型每一层 layer 的输入输出调整量化权重。**`GPTQ 就是采用这种方法。`**

`**根据量化公式的不同，可以分为线性量化和非线性量化，也可以分为对称量化和非对称量化。**`

根据量化公式是否为线性，量化可以分为线性量化和非线性量化。我们这里主要讨论线性量化。

在线性量化下，浮点数与定点数之间的转换公式如下：
Q=R/S+Z
R=(Q−Z)∗S

- R 表示量化前的浮点数
- Q 表示量化后的定点数
- S（Scale）表示缩放因子的数值
- Z（Zero）表示零点的数值

**对称量化**（如左图所示）中，量化前后的 0 点是对齐的，因此不需要记录零点。它适合对分布良好且均值为 0 的参数进行量化。因此对称量化常用于对 weight 量化。

**非对称量化**（如右图所示）中，量化前后 0 点不对齐，需要额外记录一个 offset，也就是零点。非对称量化常用于对 activation 做量化。


### 模型量化具体是怎么实现的？

对称量化中，零点 Z = 0，一般不记录，我们只需要关心如何求解 Scale。由于 weight 几乎不存在异常值，因此我们可以直接取 Scale 为一个 layer 或 block 内所有参数的**最大绝对值**，于是所有的参数都在 [-1, 1] 的区间内。随后，这些参数将找到最近的量化格点，并转化成定点数。这里需要引入 **Block-wise quantization** 的概念。通常情况，为了避免异常值（outlier）的影响，我们会将输入 tensor 分割成一个个 block，每个 block 单独做量化，有单独的 scale 和 zero，因此能使量化的精度损失减少（见下图橙色的误差部分）。

### **需要关注哪些指标？**

模型量化可以看成模型的压缩/解压过程，也可以理解成模型加密/解密的过程。

既然量化算法相当于一个压缩算法，自然我们需要关注：

- `**压缩比**`，也就是说，一种量化方法能减少多少内存/显存占用？
- `**压缩/解压缩的速度**`，这影响量化模型推理的速度，也是我们需要重点优化之处。

对于第一个关注点，当我们确定了量化精度（例如 int4），确定了量化方法，以及需要量化模型的哪些 layer，其内存和显存占用就基本确定下来了。大部分情况下，我们都只去量化 nn.Linear 层，目前几乎所有量化策略都是这么做的，而且量化模型的显存占用较少，因此我们几乎不会去考虑怎么进一步减少量化模型的体积。

对于第二个关注点，我们着重于模型 forward、backward 计算过程的解压缩速度。由于这些计算基本都在 GPU 上进行，所以我们就需要去优化 GPU 的 op 了。\


## INT8量化
在训练时，为保证精度，主权重始终为 FP32。而在推理时，FP16 权重通常能提供与 FP32 相似的精度，这意味着在推理时使用 FP16 权重，仅需一半 GPU 显存就能获得相同的结果。那么是否还能进一步减少显存消耗呢？答案就是使用量化技术，最常见的就是 INT8 量化。

简单来说， INT8 量化即将浮点数 xf 通过缩放因子 scale 映射到范围在[-128, 127] 内的 8bit 表示 xq ，即

$$x_q=Clip⁡(Round⁡(x_f∗ scale ))$$

其中 Round 表示四舍五入都整数，Clip 表示将离群值(Outlier) 截断到 [-128, 127] 范围内。对于 scale 值，通常按如下方式计算得到：

$$amax=max(abs(xf))$$ $$scale =127/amax$$

反量化的过程为：

xf′=xq/scale

下面是通过该方式实现的量化-反量化的例子：