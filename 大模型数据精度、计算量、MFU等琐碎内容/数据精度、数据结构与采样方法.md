# FP8和FP8混合精度训练
[[LLM]FP8计算在模型训练中的应用 - 知乎](https://zhuanlan.zhihu.com/p/26825649731)
采用FP8（8-Bit Floating Point）数据格式在训练和推理中能够提升计算效率，但该格式因精度问题在训练中被采用的场景不多。例如，Google公司的[Gemma模型](https://zhida.zhihu.com/search?content_id=254387907&content_type=Article&match_order=1&q=Gemma%E6%A8%A1%E5%9E%8B&zhida_source=entity)、Inflection AI公司的[Inflection2模型](https://zhida.zhihu.com/search?content_id=254387907&content_type=Article&match_order=1&q=Inflection2%E6%A8%A1%E5%9E%8B&zhida_source=entity)以及01万物等。直到最近，[Deepseek V3](https://zhida.zhihu.com/search?content_id=254387907&content_type=Article&match_order=1&q=Deepseek+V3&zhida_source=entity)使用FP8混合精度训练才正式引起了大部分人对FP8精度的关注。本文将简单梳理FP8精度训练的相关内容，围绕以下几个问题：

1. FP8的数据格式和计算优势是什么样的？
2. 应用FP8需要解决什么问题？
3. DeepseekV3等模型中是如何应用FP8的？
4. 国产卡FP8现状如何？
## 1 FP8在GPU中的应用介绍

### 1.1 FP8数据的格式

FP8数据格式是一种8位浮点数格式，常见形态：E4M3和E5M2（在英伟达的[Hopper架构](https://zhida.zhihu.com/search?content_id=254387907&content_type=Article&match_order=1&q=Hopper%E6%9E%B6%E6%9E%84&zhida_source=entity)介绍中给出），其中 E 代表指数位（Exponent），M 代表尾数位（Mantissa），两者都有一个符号位（sign）。
### 1.2 FP8的特点

从数据本身来看FP8占用的位数少，关键特点：**内存小、计算少、精度低**。

这种数据格式应用到GPU的运算中的优势：

> 性能提升：由于 FP8 的数据宽度更小，减少显存占用，降低通讯带宽要求，提高 GPU 内存读写的吞吐效率。并且在相同的硬件条件下，支持 FP8 的 Tensor Core 可以在相同时间内进行更多次的浮点运算，加快训练和推理的速度。  
> 压缩模型降低成本：FP8 的使用促使模型在训练和推理过程中进行量化，这有助于模型的优化和压缩，进一步降低部署成本。

在硬件上面目前支持的FP8的架构是NVIDIA Hopper架构之后的GPU，比如H100/H800、B200等。 以H100的架构为例，其算力吞吐是BF16的一倍：

FP8相比INT8差异点：

- 对精度影响：FP8通过混合精度策略（如在关键计算中使用更高精度）可以更好地保持模型精度；INT8需要复杂的量化校准（如量化感知训练）来减少精度损失，且在某些情况下精度损失可能大于FP8。
- 表达范围：FP8采用浮点表示，具有更宽的动态范围，能够同时表示极小值和较大值，INT8是均匀分布的整数，无法同时精确表示小数值和大数值，在小值区域的相对误差更大。
- INT8 更适合整数运算和存储效率更高的场景

### 1.3 FP8应用需要解决的问题

主要是解决FP8的精度损失问题。在应用中，通过硬件和软件的设计克服FP8的溢出问题以及转换时损失过大的问题。常见手段如下：

**引入高精度的计算累积**。在GPU上的FP8 Tensor Core中，为控制精度损失，采用以下逻辑处理：先进行低精度运算，然后用高精度进行累积。具体来说，Tensor Core内部进行低精度的乘法和累加操作，每累积一定数量的FP8结果（如128个）后，将其转换到FP32或FP16中进行存储，以避免溢出。

# 数据精度
byte、bit、KB、Kb

float32=4byte

float32 e8m23 s1(sign 1)

float16 e5m10 s1(sign 1)

bfloat16 e8m7 s1

# 混合精度训练

### 2.1.1 原理介绍

顾名思义，混合精度训练就是将多种不同的精度数据混合在一起训练，《 MIXED PRECISION TRAINING 》这篇论文里将FP16和FP32混合，优化器用的是Adam，如下图所示：

按照训练运行的逻辑来讲：

**Step1:优化器会先备份一份FP32精度的模型权重，初始化好FP32精度的一阶和二阶动量（用于更新权重）。**

**Step2:开辟一块新的存储空间，将FP32精度的模型权重转换为FP16精度的模型权重。**

**Step3:运行forward和backward，产生的梯度和激活值都用FP16精度存储。**

**Step4:优化器利用FP16的梯度和FP32精度的一阶和二阶动量去更新备份的FP32的模型权重。**

**Step5:重复Step2到Step4训练，直到模型收敛。**

我们可以看到训练过程中显存主要被用在四个模块上：

- 模型权重本身（FP32+FP16）
- 梯度（FP16）
- 优化器（FP32）
- 激活值（FP16）

### 2.1.2 三个小问题

写到这里，我就有3个小问题，**第一个问题，为什么不全都用FP16，那不是计算更快、内存更少？**

根据我们第一章的知识，我们可以知道FP16精度的范围比FP32窄了很多，这就会产生数据溢出和舍入误差两个问题（想深入了解的，请看[全网最全-混合精度训练原理](https://zhuanlan.zhihu.com/p/441591808)），这会导致梯度消失无法训练，所以我们不能全都用FP16，还需要FP32来进行精度保证。看到这里你也许会想到可以用BF16代替，是的，这也是为什么如今很多训练都是BF16的原因，至少BF16不会产生数据溢出了，业界的实际使用也反馈出比起精度，大模型更在意范围。

	节选自[全网最全-混合精度训练原理 - 知乎](https://zhuanlan.zhihu.com/p/441591808)
	使用FP16同样会带来一些问题，其中最重要的是1）精度溢出和2）舍入误差。
	1. 数据溢出：数据溢出比较好理解，FP16的有效数据表示范围为 6.10×10−5∼65504 ，FP32的有效数据表示范围为 1.4×10−45∼1.7×1038 。可见FP16相比FP32的有效范围要窄很多，使用FP16替换FP32会出现上溢（Overflow）和下溢（Underflow）的情况。而在深度学习中，需要计算网络模型中权重的梯度（一阶导数），因此梯度会比权重值更加小，往往容易出现下溢情况。
	2. 舍入误差：Rounding Error指示是当网络模型的反向梯度很小，一般FP32能够表示，但是转换到FP16会小于当前区间内的最小间隔，会导致数据溢出。如0.00006666666在FP32中能正常表示，转换到FP16后会表示成为0.000067，不满足FP16最小间隔的数会强制舍入。
	

**第二个问题，为什么我们只对激活值和梯度进行了半精度优化，却新添加了一个FP32精度的模型副本，这样子显存不会更大吗？**

答案是不会**，**激活值和batch_size以及seq_length相关，实际训练的时候激活值对显存的占用会很大，对于激活值的正向优化大于备份模型参数的负向优化，最终的显存是减少的。（这里还可以考虑梯度检查点的优化方法，能更进一步优化激活值的显存，感兴趣可以看看这个[大模型高效训练基础知识：梯度检查点（Gradient Checkpointing）](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/2338646)）。

**第三个问题，我们知道显存和内存一样，有静态和动态之分别，那么上面提到的哪些是静态哪些是动态呢？**

应该很多人都能猜到：

- 静态：优化器状态、模型参数
- 动态：激活值、梯度值

也就是说，我们其实没法特别准确的计算出我们实际运行时候的显存大小，如果在面试的时候，就可以忽略掉激活值的计算，梯度当做静态计算就好。 如果想要深度探索，指路[[LLM]大模型显存计算公式与优化](https://zhuanlan.zhihu.com/p/687226668)

## [LLM]大模型显存计算公式与优化
### 2.1 训练场景

训练显存消耗（可估算部分）主要包括：**模型参数（Model）+ 优化器状态（Optimizer status）+梯度值（Gradient）+激活值（Activation）**。

根据数值的变化，可将显存消耗分为静态/动态值。

训练过程中，模型参数、优化器状态一般不会变化，这两部分归属于静态值；

激活值、梯度值会随着计算过程发生变化，将它们归类到动态值。

**2.1.1 模型显存（Model Memory)**

模型自身所占用的显存大小与**参数量、参数类型**相关。常见类型fp32、fp16/bf16、还有int8、fp8等。

计算满足 ModelMem=TypeSize∗Params 根据不同数据类型，有如下计算公式（ 单位：GB）：

fp32=4∗params/(1024∗1024∗1024)

fp16/bf16=2∗params/(1024∗1024∗1024)

fp8/int8=1∗params/(1024∗1024∗1024)

关于模型保存的大小估算方法：

存储checkpoint（ckpt）时仅考虑模型本身，只要将显存上模型内容存储到磁盘中。举例：以1B（billion）模型为例，若采用fp32类型将其存储在磁盘上，其大小为：

Memory=4∗1∗109/(1024∗1024∗1024)≈3.725GB

1B模型需要3.725GB存储空间，进一步近似认为**1B** ≈ **4GB**，可方便作存储的估算推导，如LLama13b，大约需要52GB存储空间。注意：混合精度（Mixed-precision）最后存储的类型也是fp32，公式也适合混合精度。

**2.1.2 优化器状态（Optimizer status）**

在LLM中常见的优化器是Adam，优化器中每个参数需要一个Momentum和一个Variance状态参数，在混合精度训练中Adam还有一份**模型参数副本**。Adam参数器状态值计算公式（单位GB）：

OptMem=(4+4+4)∗Params/(1024∗1024∗1024)

其中（4+4+4）的内容：

- 模型副本 4 Bytes
- Momentum 参数 4 Bytes
- Variance 参数 4 Bytes

如果是8位优化器，则计算变为：

8BitOptMem=(4+1+1)∗Params/(1024∗1024∗1024)

- 模型副本 4 Bytes
- Momentum参数 1Byte
- Variance参数 1Byte

**2.1.3 梯度值（Gradient）**

梯度值与模型数据类型保持一致，计算如下（单位GB）：

1. fp32的模型梯度值： Memfp32=4∗params/(1024∗1024∗1024)
2. fp16或者bf16模型： Memfp16/bf16=2∗params/(1024∗1024∗1024)

**2.1.4 激活值（Activation）**

激活值的大小跟模型参数、重计算、并行策略等相关，这里我们参考Megtron论文里面给的计算公式，来求解激活值所占用的显存大小。

激活值显存消耗： s∗b∗h∗(34+5∗a∗s/h)∗L∗γ (**单位GB**)，参数说明：

- s 序列长度（sequence length), tokens的量
- b 微批量大小（microbatch size）
- h 隐藏层大小（hidden dimension size）
- a attention的头数 （number of attention heads）
- L transformer模型的层数
- λ 比例系数，当为fp16时 值等于1 / (1024 * 1024 * 1024)。


### 2.3 推理场景

推理的显存量组成成分比训练简单，有一个简单的估算公式：

总显存占用： InferMemory≈1.2∗ModelMemory

相关内容可参看这篇blog：[Transformer Inference Arithmetic | kipply's blog](https://link.zhihu.com/?target=https%3A//kipp.ly/transformer-inference-arithmetic/)

总之，通过综合求解公式可以知道模型显存消耗主要部分，能帮助我们确定显存的优化的策略。

### 3 显存优化

由于大模型的参数成倍数的增长，远超出了单GPU物理显存所能承载的范围，大模型训练必然需要进行显存优化。显存优化要么是**优化算法本身**，降低模型算法的显存消耗；要么是去**扩大显存**，通过一些置换方式获得“额外“空间，由于显存物理大小一定，我们获得额外空间的方式不外乎两种：

- **时间换空间；** 如，重计算
- **空间转移；** 如，多卡并行/offload

其中，时间换空间通常会消耗**算力、带宽**；空间转移主要是消耗I/O带宽，有一定的时延，可能会降低吞吐**。**

显存优化的过程一般是从模型算法本身到底层，可以参考的优化路径：

**多卡并行 -> 算子/数据类型 -> 消除框架副本 -> 显存管理 -> 底层API**

1、**多卡并行**：该手段相对来说是使用频率最高，且一般不会影响运算的精度，可以用2节中的计算公式为参考去设计新的TP/PP/DP/Zero/重计算的相关参数来降低显存消耗。缺点：这些方式可能会增加额外的带宽消耗。

2、**算子优化**：选取精度相同但显存消耗更低的算子/方案。缺点：一般情况下，算子优化的过程耗时较长。

3、**数据类型修改**：用低精度替换高精度数据。比如用fp16代替fp32，或者用更低的int8/int4。缺点：该方式可能影响训练收敛性/推理性能。

4、**消除框架副本**：在AI框架（如pytorch）中有些数据是一些由框架产生的中间副本，可以进行优化消除；缺点：游湖成本较大。

5、**显存管理**：通过显存管理的知识可知[[PyTorch显存管理](https://zhuanlan.zhihu.com/p/680769942)]，框架的显存管理会产生显存碎片，通过优化显存管理来优化碎片；缺点：目前可用的手段较少。

6、**底层API**： 在GPU的驱动库中/CUDA算子库中，不同API显存消耗不一样，我们可以用显存消耗更小算子去替换大显存消耗算子，比如FlashAttention；有些默认的操作会产生额外系统显存，也可以考虑替换更高版本优化后的API。

# 另一篇

数据结构

数据大小和显存占用是可以计算的

具体和网络结构、计算精度密切相关


采样方法

主要是temperature、topp、topk

以及beam search

而beam search是对贪心策略一个改进。思路也很简单，就是稍微放宽一些考察的范围。在每一个时间步，不再只保留当前分数最高的**1**个输出，而是保留**num_beams**个。当num_beams=1时集束搜索就退化成了贪心搜索。
可以发现，beam search在每一步需要考察的候选人数量是贪心搜索的num_beams倍，因此是一种牺牲时间换性能的方法。