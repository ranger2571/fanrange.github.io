文章：[笔记：Llama.cpp 代码浅析（一）：并行机制与KVCache - 知乎](https://zhuanlan.zhihu.com/p/670515231)

需要先理解 LLM 推理的过程是一个自回归的过程，每次生成一个 token 的时候需要结合前面所有的 token 做 attention 操作。也就是说前 i 次的token会作为第 i+1 次的预测数据送入模型，才能得到第 i+1 次的推理 token。

KV Cache 核心节约的时间有三大块：**1）**前面 n-1 次的 Q 的计算，当然这块对于一次一个 token 的输出本来也没有用；**2）**同理还有 Attention 计算时对角矩阵变为最后一行，和 1）是同理的，这样 mask 矩阵也就没有什么用了；**3）**前面 n-1 次的 K 和 V 的计算，也就是上图紫色部分，这部分是实打实被 Cache 过不需要再重新计算的部分。

这里还有个 softmax 的问题，softmax 原本就是针对同一个 query 的所有 key 的计算，所以并不受影响。


  
_以下描述有部分不严谨，深入研究后还需要修改：_

kv cache 部分的逻辑在代码中主要的在流程逻辑中实现的，而不是矩阵乘法部分完成的（这和前文并行部分是没有关系的）。也就是说在矩阵乘法运算逻辑部分，其实它是看不到哪些数据是从 cache 中拿出来的哪些是新算的，它只需要做好运算即可。逻辑流程部分是在 main.cpp（llama2 推理流程）中维护了 kv_cache 的 struct ，以及相关的支持操作函数（如 llama_kv_cache_seq_shift、llama_kv_cache_seq_rm 等）。

这里的 kv_cache 结构在代码注释中说是一个 ring-buffer 的设计（猜测应该为一个序列，序列 id 超限之后循环取模即可）。但是在结构体定义时并没有具体的代码，所以是存疑的。某个 [issue](https://link.zhihu.com/?target=https%3A//github.com/ggerganov/llama.cpp/issues/4251) 也提到了这个问题，并结合这个 [pr](https://link.zhihu.com/?target=https%3A//github.com/ggerganov/llama.cpp/pull/3228) 的更新日志推测理解，llama_kv_cache_??? 等操作 cache 的 cell 的 seq_id 完成一个循环空间的复用。