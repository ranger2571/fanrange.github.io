[【Transformer 基础系列】手推计算量FLOPS和训练时间 - 知乎](https://zhuanlan.zhihu.com/p/648988727)
## 0 前置知识和标记

1. 显存占用 = 参数数量 x 该参数精度占用的 bytes 数
2. 换算关系：Int8 需1 bytes, fp16 / bf16 数需 2 bytes, fp32 需要 4 bytes

- transformer 模型的层数为 l(layers)
- 隐藏层维度为 h(headsize)
- 注意力头数为 a(headnum)
- 词表大小为 v(vocab_size)
- 批次大小为 b(batchsize)
- 序列长度为 s(seq_len)

## 训练过程的显存占用
两部分：
模型的相关占用：参数、梯度、优化器状态
剩余占用：中间激活、临时占用、显存碎片
#### 模型相关
参数量的大小是$\Phi$,参数的显存占用是（fp16）$2*\Phi$,梯度是（fp16）$2*\Phi$,

优化器状态，以adam为例是一阶动量和二阶动量，分别是（fp32）$4*\Phi$,

而且要保存1份fp32的参数，所以fp32参数的显存占用是$4*\Phi$,

总的占用就是2+2+4+4+4，大概就是$16\Phi$,

1. 这部分比较固定，主要和参数量有关，和输入大小无关。
2. 在整个训练过程中都要存在显存中。 模型参数一般只能通过并行切分（Tensor Parallelism/Pipeline Parallism）能减少。优化器状态一般通过ZeRO 来减少。
#### 中间激活
激活（activations）指的是前向传递过程中计算得到的，并在后向传递过程中需要用到的所有张量。

1. 这部分比较灵活，激活值与输入数据的大小（批次大小 b 和序列长度 ）成正相关。
2. 在训练过程中是变化值，特别是 batch size 大的时候成倍增长很容易导致 OOM。
3. 可以通过重计算、并行切分策略减少。

激活计算比较简单，就是变量的shape的维度相乘，得到变量的参数量，再乘以数据精度大小，fp16是2bytes，得到一个变量的激活，然后相加得到全部激活。（**其实不好计算的是，如何找到全部的中间变量？）


## 推理过程显存占用

推理的显存占用，不存在中间激活（不需要backward），没有梯度和优化器，所以主要占用就是模型的参数占用。

一般总显存经验值估算为 1.2 倍参数量

1. **模型参数** fp16 下推理参数占 2Φ bytes
2. **KV Cache (如有)** 缓存 KV Cache 加速方法
3. **中间结果和输入数据** 比较少，一般 20% 内

### kv cache 显存分析
KV Cache 是典型的推理加速方法，推理时缓存第 n 个 token 及前计算结果，第 n+1 个 token 相当于增量计算从而加速。

输入一个prompt，经过tokennizer后，假设形状是[batch,seq_len,channel]

prefill阶段，输入的kv cache是空，输出的kv cache是输入的prompt的长度。

每个transformer block的kv cache的大小应该是，需要计算的K的长度k_len，k_len\*batch\*channel就是参数的大小，显存占用等于k_len\*batch\*channel（因为kv cache是fp8量化存储）。
