
## **Kimi 也会累**

和一般按照峰值容量规划，调度主要解决如何尽可能利用闲置资源的传统工作不同，Kimi 自从破圈之后面临的是完全不同的，如何在不断扩容还天天过载的情况下尽可能保障用户体验的问题。这里主要是我们发现由于当前的负载情况已经无法按照 [SLO](https://zhida.zhihu.com/search?content_id=245065405&content_type=Article&match_order=1&q=SLO&zhida_source=entity) 提供响应，所以与其提供一个完全无保障的长时间排队（违反 [TTFT](https://zhida.zhihu.com/search?content_id=245065405&content_type=Article&match_order=1&q=TTFT&zhida_source=entity) SLO）或者一字一顿的慢速回答（违反 [TBT](https://zhida.zhihu.com/search?content_id=245065405&content_type=Article&match_order=1&q=TBT&zhida_source=entity) SLO）这样极大伤害体验的服务，不如先暂停一小部分服务。

## **分离式架构在过载场景下的挑战**

虽然 Mooncake 大幅提升了 Kimi 能够承载的总吞吐，但 [Prefill](https://zhida.zhihu.com/search?content_id=245065405&content_type=Article&match_order=1&q=Prefill&zhida_source=entity) 和 [Decode](https://zhida.zhihu.com/search?content_id=245065405&content_type=Article&match_order=1&q=Decode&zhida_source=entity) 分离的架构在过载调度上也的确引入了一些新的挑战。

这里最重要的问题就是 Prefill 和 Decode 是分别对 TTFT 和 TBT 负责的，而且**有一个关键的时间差**。所以在坏的情况下 Prefill 前调度器觉得可以同时满足 TTFT 和 TBT，但是 Prefill 后由于 Decode 集群的过载 TBT 无法满足了。这样就进入了一个是违反 SLO 还是浪费已经花费了的 Prefill 资源的两难问题。

为了解决上述问题，一个自然的，也是被 Mooncake 采用的解决方案就是同时综合 Prefill 和 Decode 两个集群的情况，然后以两者中更高负载更危险的那个集群为去判定是否可以接受服务。

由于会因为在 Prefill 集群有空闲的情况下由于未来 Decode 集群的负载问题提前拒绝一些服务，这样的策略被我们称之为 Early Reject。

由于（存在这样一种情况），（我们的系统）会因为（删去因为？）在 Prefill 集群有空闲的情况下，由于未来 Decode 集群的负载问题，而进行提前拒绝一些服务，这样的策略被我们称之为 Early Reject。

看起来很直接，但实际部署之后我们观测到集群负载出现了奇怪的颠簸现象。可以看到 Prefill 和 Decode 集群的负载就和跷跷板一样一边上去一边下来，然后交替。

仔细分析之后发现的原因下图有一个很直观的展示。由于 Prefill 和 Decode 集群负载之间的时间差，如果简单的参考当前 Decode 集群负载去拒绝请求的话会导致 Decode 集群负载被消化的时候 Prefill 没有及时跟上，由此产生了跷跷板效应。

## **基于预测的调度**

为了解决上述的问题，我们进一步设计和上线了基于预测的调度策略。原理也很直观，能够预测未来（特别是新加入的请求完成 Prefill 阶段的时刻）Decode 集群负载的话自然就可以打消时间差的问题平滑整个系统的负载。

具体来说，对于未来的某个时刻 t，首先我们将现有的 Prefill 集群中 t 时已完成的请求加入 Decode 集群，这一点可以通过预测 Prefill 时间和排队时间来实现。然后，我们假设每个请求的 decode 时刻均为 t_d，将 Decode 集群中 t 时已结束（即decode 时间超过 t_d）的请求移除。这一步我们也在尝试通过预测每个请求实际会输出的 token 数做一个更精确的预测。最后，我们利用事先拟合的模型根据 Decode 集群中 t 时仍在处理的请求预测 TBT，作为 Decode 集群的预测负载并根据这个数据决定是否需要 Early Reject。