# 高性能计算工程师需要什么技术堆栈？
因为一直在做[GPU优化](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=GPU%E4%BC%98%E5%8C%96&zhida_source=entity)，所以来说一下对于GPU优化中所需要的技术栈。

1. **对GPU系统的深入了解**。GPU的**计算体系**，以NVIDA为例，包括SM、cuda core、tensor core、grid、block、thread、warp等一系列名词所对应的含义。GPU的**访存体系**， global memory、shared memory，register，L1cache / L2cache。**对GPU系统的了解，有点像是一个开盲盒的过程，需要你不断地打开GPU，看到里面的运行机制。**给了一份[CUDA](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=CUDA&zhida_source=entity)代码，每一行运行的时候，都需要知道GPU里面会发生什么事。

就比如**sdata[tid]=input[i]**这么一行代码，从global中取一个数，放到shared memory。Global memory是否进行了合并访存？Shared memory是不是会有bank冲突？这个数是从global memory直接走到shared memory，还是先走到register再走到shared memory，中间跟L2 cache又是啥关系？如果用向量化访存会不会更好，如果更好的话，原因是什么？L2 cache的命中率是多少？从global中取数需要多少个时钟周期，从shared memory中取数又需要多少个时钟周期？一共有多少个block，这些block是怎么被分配到SM上的？Block中多少thread，能组成多少warp，warp的schedule过程是怎么样的，warp切换需要多少时钟周期？**短短的一行代码，可以引出一系列的问题，只有深入了解了GPU系统，才能真的看明白这么一行代码。**

2. **profiling 工具的使用**。对于NV来说，主要是是[nsight](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=nsight&zhida_source=entity)工具的使用，包括**nsight system**和**nsight compute**。**首先，在系统层面**，我们需要搞明白，整个代码的瓶颈是什么？是数据IO的瓶颈，还是CPU和GPU间进行数据拷贝的瓶颈，抑或是GPU kernel计算的瓶颈，又或者是kernel太多了，kernel launch造成的瓶颈。我们需要直到程序真正的瓶颈。然后再针对性地优化IO或者GPU kernel。**随后，针对GPU kernel**，我们又需要搞清楚，是访存的瓶颈还是计算的瓶颈，如果是访存瓶颈，是否进行了合并访存，是否有bank 冲突。如果是计算的瓶颈，是什么计算造成的瓶颈，如何进行优化？这一些都需要nsight等相关的profiling工具来帮助我们找到程序优化的关键。

3. **常用的优化技巧**。这个主要是针对不同的kernel。针对**访存密集型**算子，比如reduce，它的优化技巧有那些，它是如何避免warp divergence，是如何避免bank 冲突，是怎么一步步地逼近带宽极限？针对**计算密集型**算子，比如[GEMM](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=GEMM&zhida_source=entity)，它的优化技巧又有哪些？怎么进行block、thread的分配，怎么进行分块，怎么去进行预取，又怎么消除寄存器的bank冲突，怎么提高访存的带宽等等。这里面涉及到相当多的trick，**一般而言，对reduce和GEMM进行深入优化后，基本对GPU的优化技巧就了解地七七八八了**。

4. **汇编级代码调优**。对于NVIDA而言，[PTX](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=PTX&zhida_source=entity)并不是真正执行在GPU上的汇编语言，所以本质上，外部开发者并不能完全地控制GPU中每一条指令的运行。当然之前一直有利用反向工程去解码GPU的汇编，然后通过自己写的汇编代码去逼近浮点峰值效率。这个部分已经有蛮多的工作了，像Kepler AS, Maxas,turingAs等。而对于AMD而言，[ROCM](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=ROCM&zhida_source=entity)平台目前的汇编是公开的，所以可以通过汇编级别的调优去获得极致的性能表现。但写汇编和调试就是一件非常痛苦的事情了，还是建议去写好CUDA代码或者HIP代码，然后再针对性地通过汇编代码进行微调。

5. **[TVM](https://zhida.zhihu.com/search?content_id=431342503&content_type=Answer&match_order=1&q=TVM&zhida_source=entity)等Tensor编译器**。对于传统的高性能而言，一般都是针对一个或几个benchmark，不断地进行手工调优。这样存在的最大问题就是现在矩阵或者说tensor的shape是有无穷种组合的，很可能手工调优的代码在benchmark的数据上性能很好，但是在其他shape的数据上性能就不行了。而TVM等Tensor编译器的出现目前就能比较好地解决这些问题，经过不断地训练，可以针对不同shape选择比较好的参数，得到一个非常好的效果，而且省去了很多调优的精力。