摘自[笔记：学习推理加速半年之总结与迷思 - 知乎](https://zhuanlan.zhihu.com/p/704938096)
**3）投机推理。** 投机推理其实本质上是服务系统中的一个技术点，可能和 continuous batching 和分离式推理地位相同，不同的是投机推理相对独立，也就是相对和其他部件解耦合。笔记目前有三篇：[刀刀宁：聊聊大模型推理服务之投机推理](https://zhuanlan.zhihu.com/p/699166575)，[刀刀宁：投机推理番外一：特征层 speculative decoding](https://zhuanlan.zhihu.com/p/704755926)，[刀刀宁：投机推理番外二：优化树结构](https://zhuanlan.zhihu.com/p/707596864)，其中第一篇写的时候没刹住车也是直接干到两万字以上了。

投机推理是个很有趣的技术，我认为其技术，明线上是因为大模型开销大用小模型多跑几次，暗线上其实是因为单个请求解码时面临的巨大的硬件访存墙问题，小模型多跑的几次本质上就是在组织一个更大的 batch size，在**单请求过程中搭乘 weights 读取后免费计算的顺风车**，提高硬件计算单元利用率。也就是说如果访存墙形成的计算空间可以容纳几十倍的 batch size ，那么投机推理的上限也应该是这样的加速比，而事实上当前最好的方法的加速比也就只有 5 倍以内，这是因为当下的投机方法必然因为小模型能力问题造成事实上的大量浪费。而这就是当前投机推理天然存在的问题，还需要更牛逼的范式来解决这个问题。

（所以 Batched speculative decoding 也是个能做的点，AWS 已经出了一篇 BASS 2404.15778 了，我还没来得及仔细看）