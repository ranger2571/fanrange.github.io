[模型压缩-神经网络量化基础 - 知乎](https://zhuanlan.zhihu.com/p/505570612)

GPTQ在LLM PTQ W4A16方向的地位毋庸置疑，它的出发点很朴素，就是试图最小化权重量化后和量化前的误差函数，对这个最优化问题进行求解。

```text
目标：
最小化量化前和量化后的层函数误差平方差。

推导过程： 
1.对误差函数进行泰勒展开，推导出主要由二阶海森矩阵组成项决定。
2.在限定条件下(量化/剪枝某个参数)，对二阶海森矩阵使用拉格朗日乘法求最小值。
3.计算得到在量化某个参数下，其他参数如何调整使得平方误差最小。

计算优化： 
1.每行独立计算二阶海森矩阵。
2.每行按顺序进行逐个参数量化，从而可以并行计算。
3.按block维度进行更新，对剩余参数进行延迟更新弥补。
4.对逆海森矩阵使用cholesky分解，等价消除迭代中的矩阵更新计算。
```

# 要说这个吗？
- OBD，对模型做剪枝，做了假设，只保留了haisen矩阵
- OBS，参数独立不成立，使用交叉项进行表示，

# 直接看GPTQ吧
GPTQ 的创新点有：

- OBS 采用贪心策略，先量化对目标影响最小的参数；但 GPTQ 发现直接按顺序做参数量化，对精度影响也不大。

- 这项改进使得**参数矩阵每一行的量化可以做并行的矩阵计算**，同时将时间复杂度从 O(drow⋅dcol3) 降低到 O(max{drow⋅dcol2,dcol3}) ，**在大模型环境下，这项改进使得量化速度快了一个数量级**；

- **Lazy Batch-Updates**，延迟一部分参数的更新，它能够缓解 bandwidth 的压力；
- **Cholesky Reformulation**，用 Cholesky 分解求海森矩阵的逆，在增强数值稳定性的同时，不再需要对海森矩阵做更新计算，进一步减少了计算量。