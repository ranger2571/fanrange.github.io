跑通KTransformer模型的v0.2.1版本

使用单4090即可跑通DeepSeek-R1的四比特量化版本的模型

运行效果--不确定使用了32core还是dual-socket 2\*32，效果接近官方文档的dual-socket 2\*32，输入84tokens/s，输出 9tokens/s

DP分离？

语音评测 声学模型 很强 可以写进简历中

# vllm开始接触

vllm的网络入口是两个
- 离线推理 LLM class
	- 对应了LLM Engine Class
- 在线推理
	- 对应了AsyncLLM Engine Class

## LLM Engine

功能分块--接受推理请求、进行任务规划、模型执行、输出推理结果
## [AsyncLLMEngine](https://docs.vllm.ai/en/stable/design/arch_overview.html#id6)
AsyncLLMEngine 类是 LLMEngine 类的异步包装器。

使用 asyncio 创建一个后台循环，以连续处理传入的请求。

AsyncLLMEngine 专为在线服务而设计，它可以处理多个并发请求并将输出流传输到客户端。 与 OpenAI 兼容的 API 服务器使用 AsyncLLMEngine。

在 vllm/entrypoints/api_server.py 中还有一个演示 API 服务器，可作为更简单的示例。 AsyncLLMEngine 的代码可以在 vllm/engine/async_llm_engine.py 中找到。

## 其余细节概念
- worker 是处理模型推理程序的进程，并使用一个进程控制一个加速设备（GPU）
- Model Runner 每一个worker对应一个model runner
- model 每一个model runner对应一个model

### 类的层次

## Paged Attention
vllm自建了MHA的核函数，旨在与vllm的KV cache兼容，k和v的cache存储在单独的paged attention block中，
#### MHA的输入输出指针
- 输入：q, k_cache, v_cache 
- 输出：out 指向全局内存
指针指向高维的数组

#### 概念

vec 是一组同时获取、同时计算的元素，对于q和k是使得每个线程组获取16bytes；对于v，每个线程获取16bytes？？？

这是什么意思？？

线程组：一个线程组，同时获取和计算一个q token和k token，每个线程处理token数据的一部分，


### Q

Within each warp, every thread group will fetch the same query token data, but will multiply it with different key token data.在每个warp内，每个线程组会取相同的q token，但是会和不同的k token相乘

### K
每个线程组在一次内核运行中只处理一个query token，它可能通过多次循环来处理多个k token。

每个warp 将要在多次循环中处理多个k token的blocks，来确保在内核运行后，整个线程组处理所有的上下文的tokens。

在这个上下文中，处理意味着Q与K点乘



