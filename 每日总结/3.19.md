vllm的多并发运行时，比如预计12并发，但是9并发就挂了，为什么选择调低utilization呢？等于是限制了整体的gpu使用量，留足了并发的使用空间？（因为max_model_len很高，并发开始时，还能运行，跑一会儿就跑不动了）

Flash attention 的实现
attention结构部分
causal-lm结构和prefix-lm结构的主要差别就是MASK矩阵不同

## online softmax算法
可以说是llm.c里面学习到的
，其实是一个很实用的加速手段——如何融合算子，所以在flash attention中也使用了