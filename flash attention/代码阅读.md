triton版本和cuda版本

[flash-attn代码简读 - 知乎](https://zhuanlan.zhihu.com/p/710476563)

v2代码解读
cute太难读了
[FlashAttention v2核心代码解析(一） - 知乎](https://zhuanlan.zhihu.com/p/686225377)

这个也是v2源码
[FlashAttentionV2源码解析 - 知乎](https://zhuanlan.zhihu.com/p/13905301449)


[flashattention](https://zhida.zhihu.com/search?content_id=704729254&content_type=Answer&match_order=1&q=flashattention&zhida_source=entity)的核心idea是分块算softmax，隔段时间同步更新（flashattention的1 2 3 的更新步幅是不同的，有一步一update，有最后才update分母指数；也有不同的分块方法，flashattention1是split KV，flashattention 2是split Q）

这么做的原因是，GPU global memory的带宽和读写速度都和[SRAM](https://zhida.zhihu.com/search?content_id=704729254&content_type=Answer&match_order=1&q=SRAM&zhida_source=entity)差别巨大，global memory的通信开销、从global memory读的延迟开销相比SRAM会大很多。flashattention的[KV cache](https://zhida.zhihu.com/search?content_id=704729254&content_type=Answer&match_order=1&q=KV+cache&zhida_source=entity)是放在global memory里的（其实一样也可以offload到CPU啦，个人认为区别不大），每次计算的时候加载需要的矩阵数据到SRAM。当生成序列很长的时候，加载的数据量很大，SRAM的容量是有可能不够（溢出）的

之所以累计结果而不是一次全部送进去，我觉得跟缓存驱逐策略关系很大。如果你是分块送入softmax再同步的方法，可以实现计算和驱逐同步进行；而如果你整一个巨大的softmax送进去，就需要等待，这就产生了停顿，即使先算里面的QK^T，算完了再算softmax也不行，因为w/o flashattention的情况下，cuBLAS、[CUTLASS](https://zhida.zhihu.com/search?content_id=704729254&content_type=Answer&match_order=1&q=CUTLASS&zhida_source=entity)等优化库是不支持flashattention这种分块softmax再按步骤同步结果的方法的，这是w/o flashattention的注意力计算延迟比w/ flashattention高很多的原因。不知道能不能这么认为，flashattention让矩阵运算的优化扩展到了包含softmax操作下的情况，从这个角度切入就能知道，flashattention跟kv cache是完全不同的两个优化思路。