md文件的内容主要来自于这个文章：
[FlashAttention V3 源码解析（一）—— 论文篇 - 知乎](https://zhuanlan.zhihu.com/p/18986650584)
## **FA3特性**

相对于FA2，FA3主要工作在于用 hopper 的一套全新指令重写FA2，以发挥芯片的峰值算力：

- Producer-Consumer async —— Warp-Specialization software pipeline。一些warps作为生产者（TMA），一些warps作为消费者（WGMMA）。
- softmax and GEMMs overlapping with Intra-warpgroup and Inter-warpgroup。这个非常重要，因为 softmax 会调用 [CUDA SFU](https://zhida.zhihu.com/search?content_id=252819732&content_type=Article&match_order=1&q=CUDA+SFU&zhida_source=entity)（exp）单元，这个单元的算力非常低，H100 SXM 估算大约只有 4 TFLOPS 左右，远低于正常的CUDA Core 单元。虽然整个Attention 算法的计算量主要在GEMM上，softmax这边几乎可以忽略不及，但是羸弱的 SFU 单元导致这部分理论估算的执行时间相对于GEMM部分几乎是不可忽略的。后面会详细介绍。
- FP8。相对于INT8，FP8的表示范围和精度更大，同时，相对于BF16/FP16，显存&带宽需求减半，计算吞吐增加1呗，因此，使用FP8进行推理是一个性价比非常高的事情。