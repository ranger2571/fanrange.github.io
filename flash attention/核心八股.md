参考文章：[FlashAttention核心逻辑以及V1 V2差异总结 - 知乎](https://zhuanlan.zhihu.com/p/665170554)
[[Attention优化][2w字]🔥原理篇: 从Online-Softmax到FlashAttention V1/V2/V3 - 知乎](https://zhuanlan.zhihu.com/p/668888063)

那么，2-pass算法对比3-pass算法到底有啥优势呢？好像FLOPs计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的scale，也就是$d_{i−1}^′e^{m_{i−1}−m_i}$ 。对于这个细节的理解很重要，所以这里特别写一下。

首先，我们要谨记一个基础假设：

> x值，也就是pre-softmax logits，由于需要O(N^2)的显存无法放在SRAM中。因此：  
> 1. 要么提前计算好x，保存在全局显存中，需要O(N^2)的显存，容易爆显存。  
> 2. 要么在算法中online计算，每次循环中去load一部分Q，K到片上内存，计算得到x。

Attention优化的目标就是避开第一种情况，尽可能节省显存，否则，LLM根本无法处理类似100K以上这种long context的情况。而对于第二种情况，我们不需要保存中间矩阵x，节省了显存，但是计算没有节省，并且增加了HBM IO Accesses（需要不断地load Q, K）。此时，2-pass算法相对于3-pass算法，可以减少一次整体的load Q, K以及减少一次对 xi 的online recompute，因为在2-pass的第一个pass中， xi 是被两次计算共享的。类似online-softmax这种算法，对应到Attention中的应用，就是Memory Efficient Attention（注意不是FlashAttention）。

## FA2和FA1的区别

总体来说，**V2从以下三个方面做了改进：**  

- 置换内外循环位置，同时减少非矩阵的计算量。（这两点我们在第一部分中已给出详细说明）
- 优化Attention部分thread blocks的并行化计算，新增seq_len维度的并行，使SM的利用率尽量打满。这其实也是内外循环置换这个总体思想配套的改进措施
- 优化thread blocks内部warp级别的工作模式，尽量减少warp间的通讯和读取shared memory的次数。
## Warp级别并行
讲完了thread block，我们就可以再下一级，看到warp level级别的并行了。左图表示V1，右图表示V2。不管是V1还是V2**，在[Ampere架构](https://zhida.zhihu.com/search?content_id=241701055&content_type=Article&match_order=1&q=Ampere%E6%9E%B6%E6%9E%84&zhida_source=entity)下，每个block内进一步被划分为4个warp，在[Hopper架构](https://zhida.zhihu.com/search?content_id=241701055&content_type=Article&match_order=1&q=Hopper%E6%9E%B6%E6%9E%84&zhida_source=entity)下则是8个warp。**  
  
  
在左图（V1）中，**每个warp都从shared memory上读取相同的Q块以及自己所负责计算的KV块**。在V1中，每个warp只是计算出了列方向上的结果，这些列方向上的结果必须汇总起来，才能得到最终O矩阵行方向上的对应结果。所以每个warp需要把自己算出来的中间结果写到shared memory上，再由一个warp（例如warp1）进行统一的整合。**所以各个warp间需要通讯、需要写中间结果，这就影响了计算效率。**  
  
  
在左图（V2）中，**每个warp都从shared memory上读取相同的KV块以及自己所负责计算的Q块**。在V2中，行方向上的计算是完全独立的，即每个warp把自己计算出的结果写到O的对应位置即可，warp间不需要再做通讯，通过这种方式提升了计算效率。**不过这种warp并行方式在V2的BWD过程中就有缺陷了：由于bwd中dK和dV是在行方向上的AllReduce，所以这种切分方式会导致warp间需要通讯。**


FA2的计算中，先不在每个block的每次迭代计算中执行全部的rescale操作，而是最后执行一次rescale。每次计算可以减少一次除法运算。可以这样做的原因是，只要每次迭代，确保分子部分O(1),O(2)被scale为正确值以及分母部分 ℓ(1),ℓ(2) 计算正确即可。

回忆一下FA1中的forward pass算法，我们就会发现一个诡异的事情。就是，FA1的两重循环中，是先外层循环load K, V，然后内层循环再load Q。这就会导致内层循环，每次计算的只是Qi的一部分，每次内循环的迭代都需要对Oi进行全局内存的读写。而且，一个显而易见的事实就是，在Attention的计算中，不同query的Attention计算是完全独立的。也就是说，如果外部循环是先load Q，那么就可以把不同的query块的Attention分配不同thread block进行计算，这些thread block之间是不需要通信的。没错，在FA2中，正是这样做的，对于forward pass，算法调换了循环的顺序，先load Q，再load K, V。

