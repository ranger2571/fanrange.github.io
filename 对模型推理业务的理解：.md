[聊聊大模型推理服务中的优化问题 - 知乎](https://zhuanlan.zhihu.com/p/677650022)

### **看不见的推理问题：**

实际上，大语言模型在应用于服务系统的时候，并不是只满足一个用户的一次推理需求。也就是说真实的推理服务场景中，是很多用户同时并发请求，服务器同时满足每个用户的不同长度的请求，并给出不同长度的输出。要求是每个请求的延时都尽量短，如果可以让整个系统在单位时间处理的数据更多，也就是吞吐率，那就更好了。

**特征：高并发、请求长度不同、输出长度不同。
要求：TTFT短，TPOT小**

最简单暴力的方法，用一台机器去服务一个用户，服务完一个对话再串行的服务下一个用户不就完事了吗？**也不是不行，就是太浪费了。**

多机多卡的场景，**当模型变大、同时使用的人增多的时候**，面临着什么单机单卡没有遇到的问题。

大模型的推理优化，本身是个很复杂的领域，拥有多个技术方向。所以，有一丢丢**头疼的地方是**，因为问题比较新，平时交流时会发现，大模型服务优化，大家的理解有不太一样。主要是软件栈的划分还相对比较模糊。服务层、服务化，略微有些隐形。同时，业界对这个分类的强调不是很多，我们平时看到的推理技术讨论和论文，乍一看上去，很多时候很难分清楚是哪一类问题。并且两类问题都可以用于单机系统也都可以用于分布式系统。

文本重点想讨论的是，**多请求的大模型推理服务**系统的优化问题，与之对应的是**单次请求的推理优化技术**。比如说，vLLM、Paged Attention、Continuing Batching 等技术，就是多请求服务问题。再比如说 [FlashAttention](https://zhida.zhihu.com/search?content_id=238719040&content_type=Article&match_order=1&q=FlashAttention&zhida_source=entity)、[Kernel Fusion](https://zhida.zhihu.com/search?content_id=238719040&content_type=Article&match_order=1&q=Kernel+Fusion&zhida_source=entity) 等等这些就是无关单请求还是多请求情况下都能用得到的底层优化。
 

回到我们前文所说的水杯装石头的比喻中，单请求优化就是让单个石头更小，多请求优化就是在瓶子里优化怎么放石头放的更多更好。

## 多请求优化时面临的问题主要：

服务系统核心三大件：**延时**、**吞吐率**、**容错**。也就是：确保响应时间、最大化资源利用、容错机制。同时，这也是评价优化效果的评价方向。

大模型的服务系统同样也是这三大件，而在大模型问题下，三大件问题有增无减。在大规模并发的大模型服务系统下，问题则变得更为突出。

LLM 服务的请求调度与通用 ML 服务技术具有共性，因为两者都旨在有效管理传入请求并优化资源利用。而这些共同之处包括 **[动态批处理](https://zhida.zhihu.com/search?content_id=238719040&content_type=Article&match_order=1&q=%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86&zhida_source=entity)、抢占、优先级、交换、模型选择、成本效率、负载平衡和资源分配** 等。也需要满足延迟服务水平（service level objective SLO）等通用互联网服务目标。

同时，LLM 系统还有其独特的特点，例如模型非常巨大、迭代的自回归解码机制、不可以预测的未知输出长度，以及上下文信息的状态管理，这使得 LLM 也引入了独特的挑战。

以上是在线服务系统的基本特点，其实还有一类推理问题就是离线的推理任务，比如包括用于对齐、蒸馏、分析等的推理任务。这些任务的特点是不着急，但是不要让其对资源的占用变成严重的问题，这时延时不一定是第一位需求，吞吐量大则是刚需。

另外一个不容忽略的问题就是服务系统的容错问题，故障率在分布式服务系统中本身就是极为重要的工作，每张卡每台机器每个互联或者机柜都有可能在服务过程中挂掉，因为当下 LLM 服务中有大量的混和粒度（request、group、token 等，后文会展开）的调度问题，保存实时状态与快速恢复也是一个需要重点优化的问题。

除此之外，我们还会关注内存管理方面的问题，因为在实践中，有些模型虽然能加载，但在服务运行过程中，容易在长序列请求中 OOM ，这一般都是因为对内存管理的容错机制出了问题。

### 单请求多请求优化与单机优化分布式优化的关系：

我之前在 [LLM 推理相比普通 transformer 推理有什么特殊性？](https://www.zhihu.com/question/636301026/answer/3355733107) 中的回答，当时我是将 LLM 推理等同于分布式服务系统了，因此 LLM 推理是要比普通 Transformer 推理要复杂不少的。当然，其实多请求优化，也是可以在单机上做的。

只是多请求一旦超过一定量，就一定需要扩展到分布式的。所以，很多时候大规模请求服务系统，一定是分布式的。可以说，大语言模型已经是一个庞大的系统工程了，当它遇到分布式系统时，问题又称指数级增加，必将迸发出更多闪亮的火花。

那么，既然是分布式优化，那么传统分布式服务（web 服务、图像 CNN 推理服务）已经发展与实用了很多年了，服务体验也没啥好说的，那么，当下 LLM 推理的优化问题又与他们又什么本质区别呢？简单说，其本质区别就在于不定长度（且长度差异巨大，同时不可预测）的输入和输出。

## 批处理的重要性：

批处理也就是同一份参数可以同时一次与多个计算向量共同计算（也就是speculative decoding的思路吗？），在芯片底层运算时则是读取一次参数在 SRAM 中多次计算，

在访存带宽受限的情况下，一次读取数据的延时是远长于多次计算的，所以 batch 计算时是非常节省访存时间的。

我们所熟悉的 FlashAttention 的本质方法即在 attention 计算时能够尽量使得分块计算的访存复用率最高。

并且，batch 批处理的会出现较大尺寸的矩阵乘法，在芯片底层还会有更加有效的 GEMM 矩阵加速方法，使得矩阵加速效率更高。

限制于大模型自回归方法的特点，未加优化的单请求一定是 batch size = 1 的矩阵乘法过程。我们可以将单请求的矩阵乘法 GEMM ，其实严格说是 GEMV ，在 llama2-7B 中视为 1x4096 的 X 矩阵（向量）与 4096x4096 的 Wq/k/v 的矩阵相乘，而尺寸为 batch size 为 n 的请求批次视为 nx4096 与 4096x4096 的矩阵相乘。

所以，大规模 request 调度中一个非常重要的发力点就是如何把不同的 request 中的每个 X 向量组织为 batch size 为 n 的批次，同时满足 1）n 尽量大，2）整体逻辑灵活。1）好理解，2）则是因为每个请求达到时间不同，简单粗暴的组织会造成等待的请求增加延时时间，而过于细致的组织会让整个推理服务系统的调度负担，也就是我们常说的 overhead，过重，因为完全的求解最优调度问题可能是 NP 问题。

## 大规模 Request 调度的主要的优化思路和方法：


### Request Batch Scheduling：

这里我们重点分析各种 batching 技术，也就是**如何将一大波在相对较短时间内发生同时不能确定长度的请求进行细粒度拆分后再组合调度优化**的基本方法。

第**一**个问题，我们为什么需要 batch？由于推理过程是 memory-bound，在 batch_size 较小时（b<4），模型权重加载占据主要时间花费，增加 batch_size 不会造成太大推理延迟变化，能带来成倍的吞吐量提升。在 [llm-numbers文章](https://link.zhihu.com/?target=https%3A//github.com/ray-project/llm-numbers%3Ftab%3Dreadme-ov-file)（[Jeff Dean numbers](https://link.zhihu.com/?target=http%3A//brenocon.com/dean_perf.html)）（[另一篇类似的文章](https://link.zhihu.com/?target=https%3A//medium.com/%40greg.broadhead/a-brief-guide-to-llm-numbers-parameter-count-vs-training-size-894a81c9258)）中表明通过有效的对请求进行组 batch，可以把吞吐量提升 10 倍以上，在实际场景中，理论计算和评测都证实了上述结论。为了优化服务吞吐、提升资源利用率，组batch 策略是其中最重要的部分。

第**二**个问题，像传统推理方法一样直接把同时的请求合并成一组 batch 不行吗？不可以，原因就是因为一来 request 长度不确定，二来 response 长度也不确定，三来两者还没有必然的相关性关系。这样的话，prefill 阶段没法直接合并，decode 阶段也没法合并，都对不齐。

第**三**个问题，细粒度需要多细的粒度？

第**四**个问题，如何进行再组合优化？

第**五**个问题，该过程中内存空间是否还有进一步优化的空间？

后续这几个问题就是大部分方法需要处理的核心问题了。

论文中经常出现的名称有连续批处理（continuous batching），动态批处理（dynamic batching），或者是飞行中批处理（inflight batching），我之前一直想搞清楚这几个概念的具体区别，但后来我发现没有太大必要，其实他们都是一样的大体逻辑，只是策略层面略有不同，以及细节上有区别，并且有的时候还会有叫法的差异。本质上说，他们都有一个共同特点，就是都是迭代级调度（iteration-level scheduling）的批处理。

不过，我们现在需要区分的则是静态和动态的两种方式。


显而易见，动态的 continuous batching 的好处是非常明显的，无论是在坐大巴车的时候还是在推理优化中，从整体角度看。

而实际中，prefill 阶段（黄色块）和 decode 阶段（蓝色块）的运算模式还有一些差异，decode 每次会生成一个 token 输出，而 prefill 阶段是**逻辑上**是同时输出的。并且说 prefill 阶段可以是高度并行的，是计算密集的，但为了调度灵活，有些工作可以将 prefill 拆为和 decode 阶段类似的方式，方便对齐和组织；而 decode 阶段是高度串行的，是访存密集的。而组织成为较大的 batch 之后，则可以有效的将访存密集型的计算模式转换为平衡模式的。

**（其中又与TP、EP的推理架构联系，EP可以通过提高EP并行数，提升batch，而TP则不行）

# 技术总结
接下来借用 [Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems (arxiv 2312.15234)](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2312.15234) 综述中的文本对已有的方法进行一下总结。
### 请求级别调度
早期的 [LLM 服务系统](https://zhida.zhihu.com/search?content_id=238719040&content_type=Article&match_order=1&q=LLM+%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F&zhida_source=entity)（NVIDIA Triton 上的 FasterTransformer ）仅支持请求级别的调度，这与之前的方法类似。
### token级别调度
#### ORCA - continuous batching
Orca 是在 OSDI22 应是第一个提出 continuous batching 的方法，考虑到可变的输出序列长度，将生成式 LLM 请求调度级别变为 token，它的方法是以先到先服务（FCFS）的顺序执行该粒度，Huggingface 的 TGI（Text Generation Inference）就是该方案的开源实现。
#### vllm
再就是例如 vLLM 和 RayLLM 中的连续批处理（continuous batching），以及 TensorRT-LLM 中的动态批处理。而大名鼎鼎的 PagedAttention 则是 vLLM 中实现的一种新的注意力机制（它从传统的操作系统概念如分页和虚拟内存中汲取灵感，允许 KVCache 是非连续的通过在固定大小的 page 进行分配），多说一嘴，其实 PagedAttention 是非常典型的代表了 system 领域结合硬件体系结构的研究方法的。
#### 其他的token级别调度？
FastServe 专注于作业完成时间（JCT），并涉及迭代级别的抢占，以优先处理输入长度较短的请求，而不是FCFS 。SARATHI 针对分布式推理中由变长输入请求的初始迭代引起的管道泡沫，为了充分利用 GPU 计算资源，它将输入提示分成统一的块，并在可能的情况下将块槽与其他请求的解码迭代一起使用，这也被 DeepSpeed-FastGen 称为 （ Dynamic SplitFuse ）动态分割融合所采用。S3 涉及输出序列长度预测器，并帮助在GPU内存约束条件下调度更多并发请求，以获得更大的批处理大小和更高的推理吞吐量。
##  其他的技术
额外我觉得比较有特点的几个研究细分的点，还有但不限于：

1）类似 [splitwise](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2311.18677) 的粗粒度异构流水线机制，他的方法中将 prefill 阶段放在访存相对较慢或者本身较老版本的显卡上，而将 decode 放在访存较快或者较为新发布的显卡上，以做到尽量发挥两者的硬件优势。（PD分离？？？）

2）[Adobe Nirvana](https://zhuanlan.zhihu.com/p/689685429) 利用 diffusion 模型的服务系统中不同请求在不同时间片上可能的复用机制提出的缓存加速方法，这种方法只有大规模请求系统下才会出现。

3）将量化方法和服务系统设计进行 co-design，也就是 [MIT 韩松组 2305 arxiv 的最新文章 QServe](https://zhuanlan.zhihu.com/p/697465029)，我在另外一篇博客中进行了一部分都的分析：[刀刀宁：量化那些事之 KVCache 的量化](https://zhuanlan.zhihu.com/p/691537237)。

4）结合 speculate inference 的方法，
