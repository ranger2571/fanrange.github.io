- 手撕
	- 很多题，做过，但是仍需要复习，
- nsight compute怎么用，有哪些关键指标
	- 对应你的算子实现优化内容，同步处理，***核心***，
	- 拿nsight system稍微看了一下运行的结果，主要是关注于大体上的memory和kernel运行的时间比例关系，dth和htd的memory读取关系。
- GPU没有类似的内存管理机制吗？为什么要有paged attention？（vattention，是调用GPU的api进行处理，但是开发paged的时候，api还没公布，而且vattention的）（这个可以确定了吗？）
- vllm的特色pr进行学习
	- 比如 paged attention chunked prefill***核心
	- flash infer **看了，但是似乎没什么太大的收获。我应该怎么看才好？？？
	- LightLLM 在 vLLM 的基础上，使用了更细粒度（以 Token 为单位）的 KVCache 存储，减少了不规则边界的浪费。（现在vllm v1是这样的吗？）
- Deepseek的EP推理
	- MTP和continuous batching的关系（**MTP还么学）（还么学）
- CUDA图的内容（找找知乎的文章）（稍微学了一下）
	- 了解的内容是通过对cuda的kernel进行捕获，可以在高频调用但是计算密度较小的场景进行加速，降低启动kernel的时间？还是降低了什么的时间？	
	- 那么对应的大模型的场景中，
		- 在推理过程中，prefill阶段是计算密集阶段，就相对来说不合适？而且prefill的输入的batch数、seq_len的数都是不确定的，不好使用cuda graph
		- 在decode阶段，就更合适，因为batch可以选择进行拼batch、seq就是1、所以就更容易进行graph的选择和处理

>cuda graph 的作用是减少 kernel launch 的开销。在某些场景下，如有大量的 kernel 的实际运行时间很短，甚至低于 kernel launch 的时间，这时候有一定的性能收益。

> **将模型封装为多个 CUDAGraphRunner，不同的 batch 对应不同的 CUDAGraphRunner，一个 batch 对应一个 CUDAGraphRunner，在 ModelRunner 执行模型的时，根据输入的 batch 不同，寻找匹配的 CUDAGraphRunner，如果找不到，则回退直接调用 model.forward。**
> 
> **LLM 模型中，prefill 阶段输入的 batch，seq_len， 两个维度不可控，因此只针对 generate 过程使用 cuda_graph，提前设置一批 batch，针对每个不同 batch capture 住一个 cudagraph，运行时根据输入的 shape 找到匹配的 cuda_graph_runner 即可。**


- 在混合连续批处理（mixed continuous batching）的大部分时间内，都花费在 decode 阶段，**因为这个阶段的 batch 太小了。**(这个部分的内容怎么理解？？？)