## 逻辑层面的block和硬件层面的sm、warp、thread的关系
#### 问题：
“为什么一个 block 只能在一个 SM 里执行”，一个 block 的 warp 或者说 thread 在一个 SM 里通过 shared memory 做信息交互，如果跨 SM 交互，shared memory的设计会更复杂，做不到那么大？

#### 回复​：

[徐摆渡](https://www.zhihu.com/people/9e14f91c483c4aad36d74c9cd027b6c2)
我理解，除此之外主要是软硬件模型的对应关系。硬件层面，SM 内部将线程组织成 warp，一个 warp 内的 32 个线程走 SIMT，多个 warp 由 warp scheduler 调度执行；软件模型层面，一个 block 内的线程执行同一条指令；所以 block 内的线程一定在同一个 SM 里面，否则的话不能保证执行同一条指令。

[不归牛顿管的熊猫](https://www.zhihu.com/people/09828a718a5433f59d31bddecf6ab79e)
嗯嗯是的，你说的是正确的，就是前后关系搞反了，是因为GPU设计的就是每个SM有一块独立的L1 cache+shared memory，这决定了一个block不能跨SM执行，不然一个block内的线程没法通过smem通信


# 问题集合

### **C++部分**

1、[C++程序](https://zhida.zhihu.com/search?content_id=233589285&content_type=Article&match_order=1&q=C%2B%2B%E7%A8%8B%E5%BA%8F&zhida_source=entity)的编译流程，能说到[编译器](https://zhida.zhihu.com/search?content_id=233589285&content_type=Article&match_order=1&q=%E7%BC%96%E8%AF%91%E5%99%A8&zhida_source=entity)的工作流程就更好了

2、static关键字的作用

3、指针和引用的区别

4、inline某些函数为什么可以取得性能提升

5、拷贝构造函数的参数是传值还是传引用

### **[量化](https://zhida.zhihu.com/search?content_id=233589285&content_type=Article&match_order=1&q=%E9%87%8F%E5%8C%96&zhida_source=entity)quantization部分**

1、量化可以带来哪些收益？

2、量化的误差来源有哪些？

3、[ptq量化](https://zhida.zhihu.com/search?content_id=233589285&content_type=Article&match_order=1&q=ptq%E9%87%8F%E5%8C%96&zhida_source=entity)的基本流程

4、说出你认为可以量化的算子

5、什么叫混合精度吗？

6、fp32和int8和fp16的区别是什么？越详细越好

### **CPU部分**

1、x64架构相比x86_32架构有哪些升级？

2、[cpu cache](https://zhida.zhihu.com/search?content_id=233589285&content_type=Article&match_order=1&q=cpu+cache&zhida_source=entity)是什么？cache在cpu片内还是片外？

3、你觉得CPU里面哪些东西影响着CPU的性能？

### **GPU和CUDA部分**

1、说出GPU的内存层次结构，越详细越好

2、为什么AI workload更适合用GPU来训练和推理？有没有一些AI workload更适合用CPU来训练和推理？

3、为什么一个block只能在一个SM里执行？

4、GPU cuda core可以做向量化运算吗？CPU core呢？


# 问题
## 

[如果你是一位cuda面试官，你会问哪些问题？](https://www.zhihu.com/question/10951382954/answer/90039867199)


[akkaze](https://www.zhihu.com/people/kkk-37-60)
(https://www.zhihu.com/question/48510028)

1. [ThreadsPerBlock](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=ThreadsPerBlock&zhida_source=entity)和[Blocks](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=Blocks&zhida_source=entity)的数量受哪些条件约束。
2. 理论占用率怎么计算？
3. 什么是warp，什么是warp divergence？
4. cuda的内存模型里有多少种memory，它们的位置(片上还是板上)，带宽和延迟的相对大小？
5. global memory的访存合并是什么？
6. 什么样的变量会被分配在register上？什么样的变量会被分配在[local memory](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=local+memory&zhida_source=entity)上？
7. Block是怎么被[SM调度](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=SM%E8%B0%83%E5%BA%A6&zhida_source=entity)执行的？
8. 什么是cuda core？什么是tensor core？
9. 什么是[bank conflict](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=bank+conflict&zhida_source=entity)？怎么避免bank conflict，你能想到多少方法？
10. 描述一下[Block reduce](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=Block+reduce&zhida_source=entity)的大致实现。
11. 描述一下[double buffer](https://zhida.zhihu.com/search?content_id=711122540&content_type=Answer&match_order=1&q=double+buffer&zhida_source=entity)(ping pong buffer）的大概原理和实现。
12. 什么是roofline model？什么是memory bound，什么是computation bound？
13. kernel fusion为什么能提升性能？还有哪些好处？举几个fusion的例子。
14. gpu上有分支预测吗？gpu上有指令集并行吗？
15. 常用profile工具和方法。
16. float的计算一定比int消耗更多的cycle吗(主要是加法和乘法）？
17. 常见的float格式。fp32，tf32，fp16，bf16的联系和区别？
18. ptx和sass是什么，和cuda的关系？
19. cuda上的排序和topk算法原理和实现。
20. matmul的优化，超级加分题。
21. flash attention的优化，超级加分题。

  

[牛冲：CUDA程序调优指南（一）：GPU硬件](https://zhuanlan.zhihu.com/p/84509270?utm_psn=1869338859290120192)

[牛冲：CUDA程序调优指南（二）：性能调优](https://zhuanlan.zhihu.com/p/84510732?utm_psn=1869339035295674368)

[牛冲：CUDA程序调优指南（三）：BlockNum和ThreadNumPerBlock](https://zhuanlan.zhihu.com/p/84511202?utm_psn=1869339233296207873)

[Bruce 仗剑走天涯：NVIDIA GPGPU（一）总览](https://zhuanlan.zhihu.com/p/679525399?utm_psn=1869708426973687808)

[TaurusMoon：论文解读: RooflineModel](https://zhuanlan.zhihu.com/p/693068828?utm_psn=1872366177134702592)