[关于Deepseek采用EP推理方式的一些思考 - 知乎](https://zhuanlan.zhihu.com/p/29573252335)

### 前言

前段时间Deepseek发布[Deepseek-V3](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=Deepseek-V3&zhida_source=entity)和[Deepseek-R1](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=Deepseek-R1&zhida_source=entity)模型，以其超低的训练成本和堪比顶尖闭源模型的效果引起了业界轰动。除此之外，deepseek也开源了一系列的推理和训练的高性能组建，并公布他们的推理成本和盈利空间，其超高的利润也引起了广泛的讨论。本文主要从Day6中，deepseek发布的推理系统方案，发表一些自己的想法。

### 概要

Deepseek-V3/R1采用的是[分离式架构](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84&zhida_source=entity)+DP+EP，每层有256个专家和32个冗余专家，具体配置如下：

```tex
Prefill：路由专家 EP32、MLA 和共享专家 DP32，一个部署单元是 4 节点，32 个冗余路由专家，每张卡 9 个路由专家和 1 个共享专家
Decode：路由专家 EP144、MLA 和共享专家 DP144，一个部署单元是 18 节点，32 个冗余路由专家，每张卡 2 个路由专家和 1 个共享专家
```

这里不讨论分离式架构，仅讨论EP，**因为如果不用EP的话，分离式架构没有带来本质的改变**。分离式架构也很必要，但是**它必要性不再是将计算密集和访存密集性任务分开**，而是有两点：

1. **业务层面上两种差异较大的计算行为的划分**。简单来说prefill过程对一个用户而言是千万级别tokens级别的变化，而对Decode而言则是一个token或者几个tokens（投机采样）的变化，要保持一个稳定没有计算碎片的环境，PD分离是比较好的解决方案；
2. MLA实现方式的差异，Prefill计算稠密，节省计算量使用MHA的实现方式，Decode访存稠密，同时节省QKV-project的计算量，采用MQA的实现方式，这点后面有机会专门发一篇分析一下。???这个话在讲什么？

主要内容：

- 对于超大规模的[MOE](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=MOE&zhida_source=entity)模型部署，EP如何做到低成本；
- 现阶段的EP部署方案在可靠性、可扩展性、和可维护性上面可能存在哪些挑战；

EP的核心优势主要有下面四个方面：有效的模型并行、消除了畸形矩阵的运算、单卡的通信量不受整个实例的batchsize的影响、消除memory bound的影响。EP可能的挑战将从高可靠性、可扩展性，和可维护性三方面展开。

# EP的相比TP的优势

节约成本的核心在于相同资源且单用户吞吐符合基本需求条件下，尽可能高的吞吐，（*类似于你们在做的提高并发，但是你们做的太简单了）

简单来说就是在计算资源一定的条件下，有效的服务更多的用户，即整个系统的batch-size尽可能的大。

这里做一个简单的思维实验，以Decode阶段为例，一个服务实例有18个节点，共144张[H800](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=H800&zhida_source=entity)，我们假设算子的性能都相同。如果通过TP来部署，一个实例需要16张H800，可以有9个实例，模型权重重复了9份，一份模型权重有600G左右，共需要消耗5400G。对EP而言18个节点只需要部署一个实例，模型参数近似只需要1.5份，由于每一张卡MLA和共享的专家都做DP，同时MOE接架构模型参数集中在FFN部分，所以整体来看需要用到1.5个模型参数。那么这一部分将节省4500G的显存，这部分显存可以大大的增加系统的batch-size大小。

同样的问题，我们换一种方式来做思维实验，671B+的Deepseek-V3/R1模型，一次的激活参数只有37B，激活的专家只有8个，刚好一个节点可以满足需求(671/18约等于37)，所以如果使用DP+EP的方式部署，那么本质上是一个节点部署37B大小的模型，这个大小的模型使用H800x8来部署，达到14.7k的生成阶段吞吐是合理的，这强调是使用DP+EP的方式部署，下文详细展开。

### 有效的模型并行

EP不需要对Tensor划分，**天然可模型并行**，总共256个专家，最大可以扩展到256卡并行，实际使用144卡并行，并补充冗余32个experts。

但是对于TP比较难做到，TP需要**对self-attention的Q，K，V的head划分**，以及FFN部分的两个linear进行先列后行的划分。

对SelfAttention，在DeepSeek系列模型中，用到的是MLA，q的num-head为128，Decode阶段使用MQA的实现方案（Prefill阶段使用MHA的实现方案）KV的num-head共享为1，没有办法分，即采用重复的做法，如果对q强制分，会产生畸形矩阵相乘，影响GPU的空间利用率。

所以这里必须要用DP，既可以大大节省kvcache的显存占用，同时也保证了GPU的空间利用率，这点后面会详细介绍。

到这里MLA部分两者可以做到一样，但是FFN部分就有本质的区别，其一，如果使用TP划分weight，划分少了不能起到节省权值显存占用的作用，划分多了会产生畸形矩阵，降低AI值，导致GPU的空间利用率低。其二，也是笔者认为最为关键的一个点，TP的通信量是和当前实例的batch-size成正比的，且不受TP划分粒度的增加而减少，而EP的通信量只和当前卡的macro batch size相关，且不受当前部署实例总batch size的影响，可以通过增加EP并行，不断叠加，而不影响单卡的进出通信量。


### 消除了畸形矩阵的运算

这个是MLA的DP和MOE的EP所独有的优势。对MLA而言使用的是DP并行比使用TP更加高效，这个需要结合[FlashMLA](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=FlashMLA&zhida_source=entity)的实现来说（关于FlashMLA的详解，我会在kernel系列文章中介绍）。

FlashMLA将num_head维和序列维（开了MTP的话就大于1）合并到一起进行矩阵运算，使用的TensorCore是Hopper架构下的WGMMA，该指令M维最小值是64，也就是如果q的num_head * seq_q的长度小于64，就会有冗余计算，使用DP时，num_head=128，刚好是这个值的两倍，消除冗余计算，同时将其转化为一个非畸形的矩阵，更好的发挥WGMMA的性能。

另外MLA的KVcache是共用的，K使用的全量的KVCache，V使用的是Head_size<=512部分，FlashMLA一次加载Kcache到shared-mem，V的计算复用KCache，这样可以有效提高AI值，进而增加了GPU的空间利用率。（***为什么能对KVcache进行复用，K和V存储的值是一样的吗？什么时候就开始复用了？？？？？？）


TP的问题就显而易见了，它将num_head维划分了，在fused-moe+TP=16的部署方案里，q_num_head=8，即使开了MTP且num-speculative-tokens=3，得到M=32<64，依然有一半的冗余计算。（***MTP是什么？？？）

另外DP的MLA可以将KVCache按照batch维划分到不同的卡上，大大减少了KVCache的冗余。MLA使用的低秩分解技术，所占比的权重并不多，主要权重集中在MOE部分，所以MLA做DP带来的整个实例的权值冗余量并不大。（***讲的太深入了，感觉很多不了解的地方，应该怎么补充呢？？？？）

对MOE部分，每张卡拥有3个完整的专家（2个路由，1个共享），没有进行tensor的划分，如果对每一个专家有足够的token进来，就不再是畸形矩阵运算。要获得高batchsize，根据上面的分析MLA DP + EP省出的大量显存，为帮助实现大的batchsize提供了显存空间基础。光有显存空间基础还不够，还需要将单实例总batchsize与单卡通信量解耦。

### 单卡的通信量不受整个实例的batchsize的影响

这个点应该是MOE模型推理使用EP（[All2All通信](https://zhida.zhihu.com/search?content_id=254937191&content_type=Article&match_order=1&q=All2All%E9%80%9A%E4%BF%A1&zhida_source=entity)）的精髓所在。

使用EP推理单卡通信量只和单卡的batchsize有关，如果想扩展整个推理实例的batchsize，只需要扩展EP并行的数目即可。

举一个例子（这里应该有图，后面不忙给补上），现有10张卡部署单个Decode实例，每张卡MLA部分做DP，MOE部分做EP，EP-size=10，每张卡的macro-batch-size=100，该实例总的batchsize为10 * 100，假设每个token分配top1个expert且完全的负载均衡，那么对每张卡的进出通信量都是100个token embeddings（没有开MTP），其中有10个是发向自己。（***MTP是什么？？？）

现在扩展这个实例为100张卡，MLA部分依然是DP，DP-size=100，MOE部分EP，EP-size=100，单卡的macro-batch-size=100不变，该实例的总batch-size增加为100\*100，扩大了10倍，但是对每张卡的通信量是没有变化的，依然是进出通信量100个token embeddings，其中有一个发向自己。***(为什么前面是一个这里是10个，区别在哪儿？？为什么会有这样的区别？？？)

这也是All2All通信的主要优势：进出通信量保持不变，关于这点大家应该并不陌生，DeepSpeed Ulysses序列并行也用到了这个思想。根据Deepseek发布的decode profile的配置，EP=128，TP=1，batchsize=128/gpu，他的一个decode 部署实例支持128 * 128 = 16,384个用户。（***这个配置是什么时候发布的，详细看看）

但是TP并没有这个性质，整体通信量随着整个实例的batchsize线性增加，且不受TP划分数目的影响。由于TP划分的是weight，对同一个实例里的每一张卡都有相同的输入，所以整个实例增大batchsize，那么对于实例内的每一张卡都需要增加batchsize，那么TP中的两个AllReduce的通信量都会相应的增加，并且即使增大TP并行度，其通信量也不会变小


无论是Self-Attention还是MLP都是对Reduce维度的划分，而reduce维度的划分，无论划分多少份，都是不会改变结果矩阵的大小的。举个例子：A：M * K，B：N * K，对K维做划分x份，最终会得到x个M\*N的矩阵，每个矩阵的维度都是M * N，可见TP中AllReduce的通信量不会随着TP划分数目的增加而减少，但是会随着整个部署实例batchsize的增大而增大。所以通信量的限制，TP几乎不可能进行大规模模型并行扩展，来增加单个部署实例的batchsize。


### 消除memory bound的影响

这里说的memory bound并不是bandwidth bound，这里区分一下，很多论文以及博客将这两者等价来谈，但是在本文场景下容易造成混淆。再此说明，本文中memory bound是指显存受限，导致不能加大batchsize，从而不能打满GPU算力。本文中bandwidth bound是指畸形矩阵带来的AI值低，导致计算性能bound到bandwidth上，从而不能发挥GPU算力。

回到fused-moe + TP=16的部署方案，671B的模型部署到16张H20/H800的卡上，其中一次推理的激活权值只有37B，也就是说有634B的模型参数是冗余的，一次推理中它们没有参与计算但是占用了大量的显存，导致batchsize很难打高。所以fused-moe + TP=16的部署方案有很严重的memory bound的问题。EP方案，每张卡至少一个路由Expert和一个共享Expert，每一个node的参数量近似等于它的激活（671/18～37）参数，在保证足够大的batchsize和负载均衡的条件下，每一个node的所有expert都处于一个完全激活的状态，消除了memory bound的问题，或者说把一个稀疏模型转化为一个稠密模型。


# EP的挑战

下面的分析是从数据密集性应用的三要素出发，对计算稠密的应用不一定有参考意义，毕竟智能助手服务不可用，在目前还不会对用户财产造成损失。开放讨论，希望可以得到更多的输入。

### 可靠性

做过模型训练的同学都知道，GPU存在掉卡的问题。如果说一个服务实例只有一张卡，覆盖的用户大概是30个人左右，那么即使出现了掉卡或者其他故障，这个影响可以通过快速将故障机摘除，并将后面的流量打到其他的服务实例上，当受影响的30个用户再次请求，服务就会恢复正常，用户几乎无感。这里有两点比较关键：1. 单实例影响的范围足够小；2. 有足够多的部署实例，且其他的部署实例有预留水位，或者说是在安全水位下。

但是对于EP这中部署场景，一个服务实例有22个节点，176张卡，假设一张卡出错概率是a%，那么176张卡至少有一张卡出错的概率是1 -(1-a%)^176，出错概率大大增加，假设a%=1%，那么这个结果是82.9%，相当大的一个概率值。一张卡导致一个176张卡的部署实例出现了故障，如何实现系统的可靠性呢？我们这样来思考这个问题：首先如果按照上文说的方法，将故障实例先摘除，将流量打到其他实例上，我们可以近似计算这个流量的大小，按照Deepseek发布的decode吞吐数据，一个node是14.8K tokens/s，按照每个用户20tokens/s的速度，18个node，每秒总共覆盖了14.8K * 18 / 20 =13.32 K 个用户。集群规模我们假设有2000张卡左右，可以部署11个实例，一个实例掉了以后其他实例将会增加10%的流量，如果另外10个实例的预留水位不足以支撑这10%的流量，整个系统会出现大量的排队，如果此时没有新的机器补充进来，扩大集群容量，就会导致大量的服务不可用。这还是2000张卡的集群，一张卡所带来的影响，如果是不同实例的多张卡呢。所以EP的方式部署推理服务，对GPU的稳定性提出了更高的要求。

其次176卡的实例，故障后重新拉起，再服务所需要的时间我理解要高于单卡的服务的，至少这么大模型需要部署这么多张卡上，虽然每张卡都只是部分部署权重，但是毕竟夸了22个节点，至少需要一个分布式的文件系统，那么这个时间必定不会很短。不管咋样，要保证服务可靠，这个拉取时间必须要远低于系统一天的平均故障时间，不然就会产生雪球效应。

### 可扩展性

这里可以从实例内的扩展性和实例间的扩展性。实例内，总共是256+32个experts，可扩展的空间有限，同时也受到All2All通信效率的影响，对一个同步通信而言，卡数越多，带来的通信效率和稳定性挑战就越大。实例间扩展，上文中说道，当有出错实例时，流量会打向正常实例上，此时正常实例如果不足够多或者预留水位不够高，就会出现服务不可用的情况。为避免这种情况，一个区需要部署足够多的实例，才能保证整个系统的稳定性和可用性。另外GPU做服务化，应该也是近几年的事，其稳定性相比CPU依然有一定的差距，那么这个服务冗余的buffer可能比cpu更大。同时EP部署方式，单实例就需要176张卡，之前的GPU部署服务，都是一个实例对应一张GPU，就拿搜推广场景来看，10TB的模型依然是PS Server + Dense Worker的架构，其中Dense worker仅对应一张GPU（没有做搜推广多年，如果有更新欢迎指出），其GPU问题都会收敛到单卡上。而现在的EP下的GPU服务，176张卡仅仅只能有一个实例，多卡之间的通信同步会大大增加出错的概率，需要进一步增大冗余的部署实例来保证服务的稳定性和可用性，其扩展成本也会进一步提高。千卡集群在国内已经是少数几家公司可以玩得起的，但是在现在EP部署方案下，要保证服务稳定可用，应该还是远远不够的。

### 可维护性

这里只讨论出了问题如何排查的情况。首先是如何复现问题，相比单实例单卡部署，176卡单实例的部署方案更难排查问题，且复现成本更高。如果某一个特定的输入出发了bug，对于平台需要将一个实例里各张卡的流量进行回放，来复现问题，流量的回落也是平台必不可少的能力，一方面用于数据的收集再训练，另一方面就是排查线上问题。复现了问题如何排查也是不容易的，之前单卡的实例靠人工就可以比较好的解决，因为覆盖的范围较小，问题可以比较好的收敛。但是对于176张卡的部署实例来说，如果还是通过人工来排查一个实例里的所有的卡运行情况，这个工作量是相当大的，如果不能及时解决问题，176张卡的时间成本也是相当高的。我相信deepseek采用训推一体的方案，也有这方面的考虑。但是长期来看，对于一个稳定持续的服务来说，平台上必须要完善的问题检测能力和监控数据，面对突发的异常情况，能及时的定位问题，解决问题。

# 总结

EP主要优势：

- 大大减小了模型参数冗余，节省出大量的显存空间，为大batchsize提供显存空间基础；
- 消除了畸形矩阵的运算，增加AI值，解决bandwidth bound问题；
- 单卡的通信量不受整个实例的batchsize的影响，为大batchsize提供高效通信基础；
- 通过节省出的显存带来的大batch，将各个节点打满，实现稀疏模型到稠密模型的转化；

当然也有它存在的问题，最大的问题可能是部署实例减少，且单部署实例故障概率增加，从而导致整个系统的稳定性和可用性面临比较大的挑战。